{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1171fb23",
   "metadata": {
    "id": "qSfK3TzzOeBK",
    "tags": []
   },
   "source": [
    "# **#2 Homework: Classification**\n",
    "\n",
    "**Fundamentals of Data Science - Winter Semester 2024**\n",
    "\n",
    "##### Matteo Migliarini (TA), Matteo Rampolla (TA) and Prof. Indro Spinelli\n",
    "<migliarini.1886186@studenti.uniroma1.it>, <rampolla.1762214@studenti.uniroma1.it>, <spinelli@di.uniroma1.it>\n",
    "\n",
    "---\n",
    "\n",
    "*Note: your task is to fill in the missing code where you see `\"YOUR CODE HERE\"` and the text part `\"WRITE YOUR TEXT HERE\"` part corresponding to each subproblem and produce brief reports on the results whenever necessary. Note also that a part of this missing code is also distributed in the python files in the folder `libs/`*\n",
    "\n",
    "As part of the homework, provide the answer to questions in this notebook report-like manner. \n",
    "\n",
    "After you have implemented all the missing code in the required sections, you will be able to run all the code without any errors. \n",
    "\n",
    "We kindly ask you to double-check this since **all** the delivered homework will be executed.\n",
    "\n",
    "The completed exercise should be handed in as a single notebook file. Use Markdown to provide equations. Use the code sections to provide your scripts and the corresponding plots.\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "**Submit it** by sending an email to:\n",
    "\n",
    "<migliarini.1886186@studenti.uniroma1.it>, <rampolla.1762214@studenti.uniroma1.it> and <spinelli@di.uniroma1.it> **by 29th November, 23:59**.\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "**Outline and Scores for #2 Homework:**\n",
    "\n",
    "\n",
    "* **Question 1: Logistic Regression** *(6 points)*\n",
    "  * **1.1: Log-likelihood and Gradient Ascent rule** (1 points)\n",
    "  * **1.2: Implementation of Logistic Regression with Gradient Ascent** (2 points)\n",
    "  * **1.3: Report** (3 points)\n",
    "* **Question 2: Polynomial Expansion** *(7 points)*\n",
    "  * **2.1: Polynomial features for logistic regression** (1 points)\n",
    "  * **2.2: Plot the computed non-linear boundary** (2 point)\n",
    "  * **2.4: Penalization** (4 points)\n",
    "* **Question 3: Multinomial Classification** *(9  points)*\n",
    "  * **3.1: Softmax Regression Model** (1 point)\n",
    "  * **3.2: Coding** (3 points)\n",
    "  * **3.3: Pipeline** (2 point)\n",
    "  * **3.4: Hyperparameters** (1 point)\n",
    "  * **3.5: Report** (2 point)\n",
    "* **Question 4: First approach to CNNs** *(8 points)*\n",
    "  * **4.1: Split the CIFAR-10 dataset** (1 point)\n",
    "  * **4.2: Identify and Correct Errors in the CNN Model** (3 points)\n",
    "  * **4.3: Training procedure** (2 points)\n",
    "  * **4.4: Evaluate** (1 point)\n",
    "  * **4.5: Report** (1 point)\n",
    "* **Question 5: Improve the accuracy** (BONUS) *(5 points)*\n",
    "  * **5.1: Custom model** (3 points)\n",
    "  * **5.2: Pretrained Network** (2 points)\n",
    "\n",
    "**TOTAL POINTS ARE 35, MAXIMUM GRADE IS 30**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b40d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    %pip install -qqq numpy scipy matplotlib pandas scikit-learn seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960c7ef",
   "metadata": {
    "id": "skGOzNBb3uF4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np # imports a fast numerical programming library\n",
    "import matplotlib.pyplot as plt # sets up plotting under plt\n",
    "import pandas as pd # lets us handle data as dataframes\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# sets up pandas table display\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c56914",
   "metadata": {
    "id": "_R7KkUpP3uF4"
   },
   "source": [
    "**Notation:**\n",
    "\n",
    "- $x^i$ is the $i^{th}$ feature vector\n",
    "- $y^i$ is the expected outcome for the $i^{th}$ training example\n",
    "- $m$ is the number of training examples\n",
    "- $n$ is the number of features\n",
    "\n",
    "**Let's start by setting up our Python environment and importing the required libraries:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306465a9",
   "metadata": {
    "id": "0G_LPstI3uF6",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1: **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba37beb8",
   "metadata": {
    "id": "ffcP6hxn3uF7"
   },
   "source": [
    "### **1.1: Log-likelihood and Gradient Ascent Rule** \n",
    "\n",
    "Write the likelihood $L(\\theta)$ and log-likelihood $l(\\theta)$ of the parameters $\\theta$.\n",
    "\n",
    "Recall the probabilistic interpretation of the hypothesis $h_\\theta(x)= P(y=1|x;\\theta)$ and that $h_\\theta(x)=\\frac{1}{1+\\exp(-\\theta^T x)}$.\n",
    "\n",
    "Also derive the gradient $\\frac{\\delta l(\\theta)}{\\delta \\theta_j}$ of $l(\\theta)$ and write the gradient update equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c4718",
   "metadata": {
    "id": "kWGef1atE3-I"
   },
   "source": [
    "-------------------------------------------------------\n",
    "\n",
    "**WRITE YOUR EQUATIONS HERE**\n",
    "\n",
    "- **Likelihood**: \n",
    "\\begin{align}\n",
    "L(\\theta) &= \\prod_{i=1}^n \\left( h_\\theta(x_i) \\right)^{y_i} \\left( 1 - h_\\theta(x_i) \\right)^{1 - y_i} &= \\prod_{i=1}^n \\left( \\frac{1}{1 + \\exp(-\\theta^T x_i)} \\right)^{y_i} \\left( 1 - \\frac{1}{1 + \\exp(-\\theta^T x_i)} \\right)^{1 - y_i}\n",
    "\\end{align}\n",
    "\n",
    "- **Log-Likelihood**: \n",
    "\n",
    "\\begin{align}\n",
    "l(\\theta) &= \\frac{1}{n} \\sum_{i=1}^n \\left( y_i \\log h_\\theta(x_i) + (1 - y_i) \\log (1 - h_\\theta(x_i)) \\right) &= \\frac{1}{n} \\sum_{i=1}^n \\left( y_i \\log \\frac{1}{1 + \\exp(-\\theta^T x_i)} + (1 - y_i) \\log \\left(1 - \\frac{1}{1 + \\exp(-\\theta^T x_i)}\\right) \\right)\n",
    "\\end{align}\n",
    "\n",
    "- **Gradient of log-likelihood** (slide 5 p. 20):\n",
    "\n",
    ">To find the gradient of the log-likelihood with respect to the parameters $\\theta$, we first compute the derivative of the sigmoid function:\n",
    " \n",
    "1. For the term $y_i \\log h_\\theta(x_i)$:\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta \\theta_j} \\left( y_i \\log h_\\theta(x_i) \\right) &= - y_i \\frac{1}{h_\\theta(x_i)} \\frac{\\delta h_\\theta(x_i)}{\\delta \\theta_j} \\\\\n",
    "\\end{align}\n",
    "> - Since $h_\\theta(x_i) = \\frac{1}{1 + \\exp(-\\theta^T x_i)}$, we have:\n",
    "\\begin{align}\n",
    "\\frac{\\delta h_\\theta(x_i)}{\\delta \\theta_j} &= h_\\theta(x_i) (1 - h_\\theta(x_i)) x_{ij}\n",
    "\\end{align}\n",
    "> -  Therefore, the derivative of the first term is:\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta \\theta_j} \\left( y_i \\log h_\\theta(x_i) \\right) &= y_i (1 - h_\\theta(x_i)) x_{ij}\n",
    "\\end{align}\n",
    "2. For the term $(1 - y_i) \\log (1 - h_\\theta(x_i))$:\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta \\theta_j} \\left( (1 - y_i) \\log (1 - h_\\theta(x_i)) \\right) &= - (1 - y_i) \\frac{1}{1 - h_\\theta(x_i)} \\frac{\\delta (1 - h_\\theta(x_i))}{\\delta \\theta_j}\n",
    "\\end{align}\n",
    "> - Using \n",
    "\\begin{align}\n",
    "\\frac{\\delta h_\\theta(x_i)}{\\delta \\theta_j} &= h_\\theta(x_i) (1 - h_\\theta(x_i)) x_{ij}\n",
    "\\end{align}\n",
    "> - We have:\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta \\theta_j} \\left( (1 - y_i) \\log (1 - h_\\theta(x_i)) \\right) &= - (1 - y_i) h_\\theta(x_i) x_{ij}\n",
    "\\end{align}\n",
    "> - Combining the derivatives of the two terms, we get:\n",
    "\\begin{align}\n",
    "\\frac{\\delta l(\\theta)}{\\delta \\theta_j} &= \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - h_\\theta(x_i) \\right) x_{ij}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- **Gradient update equation**: \n",
    "For  $j=0,...,n$:\n",
    "\\begin{align}\n",
    "\\theta_j = \\theta_j + \\alpha \\frac{\\delta l(\\theta)}{\\delta \\theta_j} &= \\theta_j + \\alpha \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - h_\\theta(x_i) \\right) x_{ij}\n",
    "\\end{align}\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69909e04",
   "metadata": {
    "id": "4N20uGxT3uGA"
   },
   "source": [
    "### **1.2: Logistic regression with Gradient Ascent**\n",
    "\n",
    "Define the sigmoid function `sigmoid`, then define the `LogisticRegression` class with the relative methods necessary to make predictions on an input, compute the log-likelihood and update its parameters. \n",
    "Then define a function that takes in input such $X$, $y$ and the predictions $\\hat{y} = g(\\theta^{T}x)$ and computes the gradient of the log-likelihood.\n",
    "Finally implement a function that takes in input such class and performs the training loop with the specified hyperparameters.\n",
    "\n",
    "Translate the equations you wrote above in code to learn the logistic regression parameters, $x^{(i)}_1$ and $x^{(i)}_2$ represent the two features for the $i$-th data sample $x^{(i)}$ and $y^{(i)}$ is its ground truth label.\n",
    "\n",
    "*Hint: even though by definition log likelihood and gradient ascent are defined by summations, for numerical stability it is advised to use the mean operation.*\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "**Fill in the code in `libs/models/logisic_regression.py`, `libs/math.py/sigmoid()` and `libs/optim.py`**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4829929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.models import LogisticRegression\n",
    "from libs.optim import fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca378a",
   "metadata": {
    "id": "c2q2DZXF3uGB"
   },
   "source": [
    "**Check your grad_l implementation:**\n",
    "\n",
    "`LogisticRegression.log_likelihood` applied to some random vectors should provide a value for `output_test` close to the `target_value` (defined below).\n",
    "In other words, `error_test` should be close to 0.\n",
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df146f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3UT5wav3uGB",
    "outputId": "e71fbb2c-8abf-4cf8-e927-23fef4a95dde"
   },
   "outputs": [],
   "source": [
    "target_value = -1\n",
    "np.random.seed(1)\n",
    "output_test = LogisticRegression.likelihood(np.random.random(100), np.random.randint(0, 2, 100))\n",
    "error_test = np.abs(output_test - target_value)\n",
    "print(\"Error: \", error_test)\n",
    "assert error_test < 0.2, \"The output is not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876a9e2",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" width=800/>\n",
    "\n",
    "Now you'll load a dataset of penguins data. The dataset contains three species of penguins (Adelie, Gentoo and Chinstrap). Your goal will be to classify a penguin species based on their bill's length and body mass. First we'll load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"assets/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee1a83",
   "metadata": {},
   "source": [
    "We want to train a classifier capable of understanding the difference between Adelie and Gentoo solely based on their bill's length and body mass. Thus in order to preprocess the data we:\n",
    "1. Drop all the items with null data.\n",
    "2. Remove the third species (Chinstrap) from the dataset.\n",
    "3. Select the features we're interested in (`bill_length`, `body_mass`).\n",
    "4. Select the label data and encode it in the values 0 and 1.\n",
    "\n",
    "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png\" width=\"500\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data = data[data[\"species\"] != \"Chinstrap\"]\n",
    "X = data[[\"bill_length\", \"body_mass\"]]\n",
    "y = data[\"species\"].map({\"Adelie\": 0, \"Gentoo\": 1}).values\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3cb402",
   "metadata": {},
   "source": [
    "It is recommended to normalize data when using machine learning techniques, so now normalize $X$ to have $\\mu=0, \\sigma=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "###             YOUR CODE HERE           #####\n",
    "##############################################\n",
    "\n",
    "# Copy X to avoid modifying the original data and avoid warnings\n",
    "X = X.copy()\n",
    "\n",
    "# Standardize the data, by substracting the mean and dividing by the standard deviation\n",
    "X[\"bill_length\"] = (X[\"bill_length\"] - X[\"bill_length\"].mean()) / X[\"bill_length\"].std()\n",
    "X[\"body_mass\"] = (X[\"body_mass\"] - X[\"body_mass\"].mean()) / X[\"body_mass\"].std()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057099fb",
   "metadata": {},
   "source": [
    "We add a column of 1's to $X$ to take into account the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118726ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"bias\"] = 1\n",
    "# Reordering columns to have the bias term first (convention)\n",
    "X = X[[\"bias\", \"bill_length\", \"body_mass\"]] \n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6c591",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Now you'll use the class defined above to train a logistic regression model on classifying a group of penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70727f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajh8uvxR3uGB",
    "outputId": "a3ea1017-fd6e-4e2c-83bb-e337cc9b2ef0"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LogisticRegression(num_features=X.shape[1])\n",
    "\n",
    "# Run Gradient Ascent method\n",
    "n_iter = 50\n",
    "log_l_history, _ = fit(model, X, y, lr=0.5, num_steps=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ba43d",
   "metadata": {
    "id": "MusdHuGZ3uGC"
   },
   "source": [
    "Let's plot the log likelihood over different iterations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b2578",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "5BFYiF543uGC",
    "outputId": "7cf633f9-3c86-472f-9954-350acc5a5fb3"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(log_l_history)), log_l_history, \"b\")\n",
    "plt.ylabel(\"log-likelihood(Theta)\")\n",
    "plt.xlabel(\"Iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333bcd8",
   "metadata": {
    "id": "pYd890o33uGC"
   },
   "source": [
    "Plot the data and the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(data=X, x=\"bill_length\", y=\"body_mass\", hue=data[\"species\"])\n",
    "\n",
    "x_range = np.linspace(X['bill_length'].min(), X['bill_length'].max(), 100)\n",
    "theta_final = model.parameters\n",
    "y_range = -(theta_final[0] + theta_final[1] * x_range) / theta_final[2]\n",
    "plt.plot(x_range, y_range, c=\"red\")\n",
    "\n",
    "plt.xlim(X['bill_length'].min() - 0.2, X['bill_length'].max() + 0.2)\n",
    "plt.ylim(X['body_mass'].min() - 0.2, X['body_mass'].max() + 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = ((model.predict(X) > 0.5) == y).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "assert accuracy > 0.6, \"The accuracy is too low\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad244e",
   "metadata": {
    "id": "FgyCuwL3E3-P"
   },
   "source": [
    "### **1.3: Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422a9aa",
   "metadata": {
    "id": "YyHpS-us3uGD"
   },
   "source": [
    "1. Are we looking for a local minimum or a local maximum using the gradient ascent rule? \n",
    "2. You have implemented the gradient ascent rule. Could we have also used gradient descent instead for the proposed problem? Why/Why not?\n",
    "3. Let's deeply analyze how the learning rate $\\alpha$ and the number of iterations affect the final results. Run the algorithm you have written for different values of $\\alpha$ and the number of iterations and look at the outputs you get. Is the decision boundary influenced by these parameters change? Why do you think these parameters are affecting/not affecting the results?\n",
    "4. What happens if you do not normalize the data? Try to run the algorithm without normalizing the data and see what happens. Why do you think this happens?\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAGwCAIAAAA2YrYSAAAAAXNSR0IArs4c6QAAIABJREFUeAHt3QtcVGX+P/Az3B1gQBBU8EaJZGpqF5RMDC3FdlfL0oK8pOVuZqVtuaK5sZsRtbWaVrqZl0pdc/Vn2j8vZGoqIojiDQFBRQXvIiCiIMx8/x5nGMYDPDPD3M458zmvfdU5Z57zPN/n/UzPd59h5hyOsEEAAhCAAATkKMDJsVPoEwQgAAEIQICQ4fAmgAAEIAABeQogw8lzXNErCEAAAhBAhsN7AAIQgAAE5CmADCfPcUWvIAABCEAAGQ7vAQhAAAIQkKcAMpw8xxW9ggAEIAABZ8xwarW6qKiorKysHBsEIAABCEhZoKysrKioSK1WN5rOnTHDFRUVcdggAAEIQEAuAkVFRchwOoGysjKO44qKiqT8f1wQOwQgAAEIlGtXLGVlZchwOoHy8nKO48rLyxsVwUkIQAACEJCKAHs+d8ZPKdkiUhlXxAkBCEAAAuz5HBkO7xAIQAACEJCqADKccOTYIsLSOIYABCAAAbEKsOdzrOHEOm6ICwIQgAAEjAkgwwmF2CLC0jiGAAQgAAGxCrDnc6zhxDpuiAsCEIAABIwJIMMJhdgiwtI4hgAEIAABsQqw53Os4cQ6bogLAhCAAASMCSDDCYXYIsLSOIYABCAAAbEKsOdzrOHEOm6ICwIQgAAEjAkgwwmF2CLC0jiGAAQgAAGxCrDnc6zhxDpuiAsCEIAABIwJIMMJhdgiwtI4hgAEIAABGwjU1ND27ZbWy57PsYaz1BfXQwACEICAWQJHj9K771Lr1sRxdPCgWZcKCyPDmSciLI1jCEAAAhCwhsCVKzR/Pj3yCJ/YtP9r1YrWrbOoamQ4IR9bRFgaxxCAAAQgYIHA7du0YQM99xy5u+sSm5sbPfssrV9P1dUW1Hv3UvZ8jk8pLfXF9RCAAAQg0KjA0aP0zjsUFFS/aHv4YZo3jy5fbrR4c04iwwnV2CLC0jiGAAQgAAFzBEpLaeFCeuyx+sQWHEx//SsdPmxOLaaVZc/nWMOZpohSEIAABCDAFFCrads2evll8vKq/zRyxAj6+We6fZt5pQUvIsMJ8dgiwtI4hgAEIAABpsCZM/ThhxQWVr9o69aN5syhS5eYl1njRfZ8jjWcNYxRBwQgAAHnE6iuprVrKTaWFApdblOp6PXXad8+0mjsxIEMJ4RmiwhL4xgCEIAABO4VyMujadMoOLh+0RYTQytWUGXlveVsf8Sez7GGs/0IoAUIQAACshCorKQffqD+/esTW5s2NGMGnTjhsO6JOsOVlJTEx8f7+vr6+flNmDChoqKiUacBAwZwBttf/vIXfbF9+/YNHDjQz8/P399/8ODBhw4d0r/U1A5bpKmrcB4CEICA0wocOkRvvEF+frrc5uJCf/wj/yu3mhoHk7Dncwev4WJjY3v27Jmenr579+7OnTvHxcU1qjVgwICJEydeqNvKy8u1xSoqKgICAl555ZW8vLzs7Oznn3++devWt419a4ct0mgAOAkBCEDACQUqK2npUurTp37R1qkTzZ5NxcViwWDP547McDk5ORzHZWZmaqk2b96sUCjOnTvXUG7AgAFTpkxpeD4zM5PjuLNnz2pfOnLkCMdxBQUFDUsanmGLGJbEPgQgAAHnFDhyhCZPrl+0ubnRyJH066+kVovLgz2fOzLDLVmyxN/fX69VU1Pj6uq6rrGblA0YMKBVq1aBgYHdunVLSEiorPtr5vXr1wMDAxMTE6urq2/evDllypSuXbvWNLZsrqqqKq/bioqKOI7TLwT1AWAHAhCAgJML3LxJ331HUVH1i7b77qPkZLp4UaQw4s1wSUlJXbp0MWQLCgpasGCB4Rnt/jfffLNly5YjR46sWLEiNDT0ueee05c5evTo/fff73J3i4iIOH36tP4lw53ExESDP+Txu8hwhj7YhwAEnFwgN5emTCF/f11uc3Oj558X46JNMEyOzHDTp08X5BX9YW5urukZzrBL27Zt4zjuxN3v7ty8eTMyMnLs2LH79u3bu3fv888/361bt5s3bxqW1+5jDdfQBGcgAAEI3L5Na9ZQTEz9oq1TJ0pKogsXpGHjyAx3+fLl3Ca26upq0z+lNJS+ceMGx3FbtmwhosWLFwcHB6vrPhiurq5WKpWrVq0yLN9wny3SsDzOQAACEJCfQFER/f3v1LatLre5uNCwYbR5s+j+0saWZ8/njvw7nPabJvv379d2ICUlpalvmhj2MDU1leO4w3dv4Tl//vw2bdpo6n49X1NT4+3tvXLlSsPyDffZIg3L4wwEIAAB2Qio1ZSSwj+8xtVVl9tat6b336czZyTZRfZ87sgMR0SxsbG9e/fOyMhITU0NDw/X/1qguLg4IiIiIyODiE6cOPHhhx/u37+/sLBww4YN9913X3R0tHYocnNzPT09J02alJOTk52dPXr0aD8/v/Pnz7MHii3CvhavQgACEJCoQGkpffEFhYfXfyA5YACtXm2Fh7Q5EIQ9nzs4w5WUlMTFxfn4+KhUqvHjx+t/8V1YWMhx3I4dO4jo7Nmz0dHRAQEBnp6enTt3njZtmuGXRH799dd+/fr5+fm1bNly4MCBe/fuNWrNFjF6OQpAAAIQkJbAkSP0l7+Qt7cut6lU9NZbdOyYtDrReLTs+dzBGa7xkG18li1i48ZRPQQgAAE7CWi/RTJgQP2irVs3+s9/qImbR9kpKus2w57PkeGsq43aIAABCDhe4OJF/s4joaG63Obqyn/1f8cO+93y324EyHBCaraIsDSOIQABCEhHYP9+GjuWPDx0uS04mP8WSd19n6TTDZMjZc/nWMOZDImCEIAABMQqUFND//sf9etX/4FkZCQtX05VVWKN2EpxIcMJIdkiwtI4hgAEICBigatX+btqtWuny21ubhQfT+npIo7YqqGx53Os4ayKjcogAAEI2Evg6FF67TXy8tLltqAg/hfcjd273l4BOaIdZDihOltEWBrHEIAABMQkoFbTxo301FP1H0j27s3fLvnWLTFFaa9Y2PM51nD2Gge0AwEIQMAygcpK/rv+Dzygy20uLvTCC7R7twy/IWm6EzKc0IotIiyNYwhAAAKOFjh/nv9KZGCgLrepVPTuu9TEk1QcHat922fP51jD2Xc00BoEIAABcwSysmjMGHJ31+W2sDD+zlvXr5tThazLIsMJh5ctIiyNYwhAAAJ2F9BoaNMmGjiw/o9tTzxB69ZRba3dQxF3g+z5HGs4cY8eooMABJxMoKqKli2jbt10uc3VleLiaN8+J1MwubvIcEIqtoiwNI4hAAEI2EXg2jX+l236B7b5+vJ/bJPoQ23sAsY3wp7PsYaz20CgIQhAAAKNCxQW0pQp9ff+Dw2lf/2LysoaL4yzhgLIcIYa/D5bRFgaxxCAAARsJnDwIP8hpP5hpA89RD/8IO0HttmMqvGK2fM51nCNq+EsBCAAAdsJaDS0bRsNGVL/RZKnn+Yfva3R2K5NedaMDCccV7aIsDSOIQABCFhPoLaW1qyhxx7T5TYXF34Nd/Cg9RpwsprY8znWcE72dkB3IQABBwncukWLFlF4uC63eXnR5Ml06pSDopFLs8hwwpFkiwhL4xgCEICAZQLl5fTpp9SmjS63tWxJs2bRpUuWVYqr7wqw53Os4fA2gQAEIGArgStX+Pv9+/vrclu7djRnDlVU2Ko5J6wXGU446GwRYWkcQwACEDBfoLiY3nmHlEpdbouI4H/HXV1tfkW4ginAns+xhmPi4UUIQAACZgoUFNDEieThocttDz9Ma9fibltmIppcHBlOSMUWEZbGMQQgAAHTBI4e5Z+v7eKiy23R0bRlC34AYJpdc0ux53Os4ZrriusgAAEI1AlkZdFzz+kSG8fR0KH8Y9uw2UEAGU6IzBYRlsYxBCAAgaYF0tPpD3/Q5TaFgn8kaVZW06XxirUF2PM51nDW9kZ9EICAcwjs3k2DB+tym4sLvfwyHTvmHD0XUy+R4YSjwRYRlsYxBCAAAQMB7Q23nnxSl9tcXemVVyg/36AEdu0owJ7PsYaz41CgKQhAQMoCGg1t3Ur9+ulym7s7/fnPuCmJg0cUGU44AGwRYWkcQwACTi8gyG2envwNt86edXoXEQCw53Os4UQwRAgBAhAQq4BGQ7/9Rk88oVu3eXryD3I7d06s4TpfXMhwwjFniwhL4xgCEHBKAe3f2/r3r89tb7+N3Ca6twJ7PscaTnQDhoAgAAGHC2zfTtHRyG0OHwfjASDDCY3YIsLSOIYABJxJIDWV9N+T9PSkt96i4mJn6r/U+sqez7GGk9p4Il4IQMA2ApmZFBurW7d5eNCbbyK32QbaqrUiwwk52SLC0jiGAATkLnD4MA0frsttbm78bwDwPUmpjDl7PscaTirjiDghAAHrC+Tk0KhRutzm4kLjxtHJk9ZvBTXaTgAZTmjLFhGWxjEEICBHgZMnaezY+ucAvPgi5ebKsZ9y7xN7PscaTu7jj/5BAAL3Cpw/T5MmkZubbun27LN0+PC9JXAkHQFkOOFYsUWEpXEMAQjIReDaNZo+nVq00OW2wYNp3z659M1Z+8Gez7GGc9b3BfoNAWcSuHGDPv6Y/P11uS0qin7/3Zn6L9++IsMJx5YtIiyNYwhAQMoC1dX01VfUurUut3XvThs24LnbUh7Re2Nnz+dYw92rhSMIQEAuAmo1LV9OYWG63BYWxh/W1sqle+jHXQFkOOEbgS0iLI1jCEBAagIaDW3ZQj176nJbmzb09ddUXS21biBeEwTY8znWcCYQoggEICAdgf37aeBAXW5TqSgpiW7ckE70iNRMAWQ4IRhbRFgaxxCAgEQETp6kl17S5TYPD3rnHbpyRSKhI8zmCrDnc6zhmuuK6yAAAdEIXL7M3yLZ3Z1PbwoFjR5NhYWiCQ6B2FIAGU6oyxYRlsYxBCAgYoHKSpo9m3x9dUu3IUPo4EERh4vQrC3Ans+xhrO2N+qDAATsIlBbS8uWUUiILrc9/DD/MG5sziaADCcccbaIsDSOIQAB8Qls3Vr/VcmOHem//yW1WnxRIiLbC7Dncwev4UpKSuLj4319ff38/CZMmFBRUdEUSFpaWkxMjFKp9PX17d+//82bN7UlTa9BXzNbRF8MOxCAgAgFsrPpmWd06zY/P/rsM7p1S4RhIiQ7CbDncwdnuNjY2J49e6anp+/evbtz585xcXGNqqSlpalUquTk5Ozs7Ly8vNWrV1dVVWlLmliDYbVsEcOS2IcABMQjcOEC/+Q2Fxc+vbm50dtv09Wr4okOkThGgD2fOzLD5eTkcByXmZmphdm8ebNCoTh37lxDpz59+syaNavhedNrMLyWLWJYEvsQgIAYBLRfJ/Hx0S3dRoyg/HwxxIUYHC/Ans8dmeGWLFni7++vF6qpqXF1dV23bp3+jHbn0qVLHMfNnz8/KioqODg4Ojp69+7d2pdMrIGIqqqqyuu2oqIijuPKy8sFDeEQAhAQm4BGw/+NrX17XW6LjKS6//rFFinicYyAeDNcUlJSly5dDFWCgoIWLFhgeIaI9u7deyfDBQQELF26NCsra+rUqR4eHvl3/y+ciTXcqTAxMZG7d0OGEzjjEAJiE0hPp759dbmtQwdatQp3TBbbEDk+HkdmuOnTp9+bVuqPcnNzTcxPe/bs4ThuxowZessePXokJCQQkYk1YA2np8MOBCQhcPYsvfyyLrd5e9NHH1Hdd8skET6CtJ+AIzPc5cuXc5vYqqurTfyM8dSpUxzHLV++XG82atSo+Ph4IjKxBv2F2h22iKAwDiEAAXsK3LmHZGKi7iGlCgWNH0+N/WnenhGhLVELsOdzR/4dTvs9kf3792v9UlJSGv2miUajCQkJMfymSa9evbRLOhNrEIwPW0RQGIcQgIB9BLQPuwkN1S3d+venurnBPu2jFUkKsOdzR2Y4IoqNje3du3dGRkZqamp4eLj+1wLFxcUREREZGRla8rlz56pUqjVr1hQUFMyaNcvLy+vEiRPal5qqgTFWbBHGhXgJAhCwkcC+ffV/cuvUidaswZ/cbCQtt2rZ87mDM1xJSUlcXJyPj49KpRo/frz+F9+FhYUcx+3YsUM/GsnJye3atVMqlVFRUfrvUhJRUzXoL2y4wxZpWB5nIAAB2wlcvEgTJvC3S+Y48vGhjz/GL7hthy3DmtnzuYMznEO82SIOCQmNQsAJBW7fpjlzSKXSfSw5Zgz+5OaE7wJLu8yez5HhLPXF9RCAQDMEUlKoa1ddbnvkEdqzpxl14BIIEDKc8E3AFhGWxjEEIGBVgZMnafhwXW4LCqJvv6XaWqs2gMqcSYA9n2MN50zvBfQVAg4VqKykWbPI05NPb66uNHUqlZY6NCA0Ln0BZDjhGLJFhKVxDAEIWCyg0dD69dSxo27p9tRTdOyYxZWiAggQPqVs8CZAhmtAghMQsKFAQQENHarLbe3b09q1+CWADbWdrWr2fI5PKZ3t/YD+QsB+ApWV9Pe/k4cHn97c3WnGDLpzyxJsELCiADKcEJMtIiyNYwhAwHwBwceSTz9NeXnm14IrIGBMgD2fYw1nzA+vQwACZgqcOFH/GG58LGkmHoqbJ4AMJ/RiiwhL4xgCEDBZoKqKPvxQ921JfCxpMhsKNl+APZ9jDdd8WVwJAQgYCmzbRl261H9bEh9LGuJg30YCyHBCWLaIsDSOIQABYwIXL9Lo0brc1ro1HlVqzAuvW0+APZ9jDWc9adQEAecTUKtp4ULy9+fTm0JBkyfjR9zO9yZwaI+R4YT8bBFhaRxDAAJNCBw8SH366JZuDz9M+/Y1UQ6nIWAzAfZ8jjWczeBRMQTkK1BRQe+8Qy4ufHrz9aX583FvSfkOtrh7hgwnHB+2iLA0jiEAgXsFfvmFOnTQLd1GjcIjb+7VwZF9BdjzOdZw9h0NtAYBKQtcuECjRulyW6dOtHmzlDuD2GUhgAwnHEa2iLA0jiEAASK1mn/MjfYbJS4u9N57uP8W3haiEGDP51jDiWKQEAQExCyQm0vR0bql2yOP0IEDYg4WsTmXADKccLzZIsLSOIaAEwtUVdE//6m7dbJSSXPmUE2NE3Og6+ITYM/nWMOJb8QQEQTEIZCWRg8+qFu6DR1KhYXiCAtRQMBAABnOAOPuLltEWBrHEHA+gYoKmjKF/wU3x1FwMO5R4nzvAOn0mD2fYw0nnZFEpBCwi8Cvv1KnTrql27hxVFJil1bRCASaJWCFDJeTk/PBBx/ExMTcd999bdq06dGjx9ixY1euXFlVVdWskBx8EVvEwcGheQg4TuDaNZowQZfbOnakLVscFwpahoBpAuz53Mga7sCBA4MGDfL09Bw4cGBCQsL8+fO//fbbTz/9dMyYMZ06dQoICPjkk08kl+fYIqapohQE5Cawbh21acOnN4WC3nqLKirk1kH0R5YC7PncSIbr1KnT119/XVpa2ihNWlraiy++mJSU1Oiroj3JFhFt2AgMAjYSuHiRRo7ULd0iIig11UbtoFoIWF+APZ8byXC3b982GpEpZYxWYs8CbBF7RoK2IOBYAY2GVqyggAA+vbm60syZdOuWYyNC6xAwT4A9nxvJcOY1JZHSbBGJdAJhQsBSgQsXaNgw3dKtd286eNDSCnE9BOwvwJ7PzchwN27c2Lhx48KFC+cZbPbvj+UtskUsrx81QEDkAtqlW8uWfHpzd6ePPiITPqwReZ8QnpMKsOdzUzNcVlZWmzZtVCqVq6trUFCQQqHw9vYOCwuTIipbRIo9QswQMF3gwgUaPly3dHv4YTpyxPRLURICohNgz+emZrgBAwZMnDhRrVb7+PicPHny7Nmz0dHR//d//ye67poQEFvEhApQBAKSFNBoaOVK3V/d3N1p9mws3SQ5jgjaUIA9n5ua4fz8/PLy8ojIz88vJyeHiNLT0yMiIgxbkso+W0QqvUCcEDBL4MIFevbZ+r+6HT5s1tUoDAGRCrDnc1MzXKtWrfLz84koPDx8y90fgubm5iqVSpF2mhkWW4R5KV6EgCQFfvyxfun24YdYuklyEBF0owLs+dzUDPf000+vXLmSiF577bXIyMgVK1YMGTIkMjKy0SZFfpItIvLgER4EzBK4erX+maW9exOWbmbpobD4BdjzuakZLjMzc/v27UR06dKlIUOG+Pr6Pvzwwwel+f1itoj4RxQRQsBEgY0bdbcpcXWlxEQs3UxkQzEpCbDnc1MznJR6bCxWtoixq/E6BCQgcP06TZyo+6tb166UmSmBmBEiBJohwJ7PTc1wMTExgnt3lZeXx8TENCMgh1/CFnF4eAgAAhYK7NxJYWF8elMo6J136OZNC+vD5RAQrwB7Pjc1wykUikuXLhn28tKlS25uboZnpLLPFpFKLxAnBBoK3LpF776re65bx460Y0fDIjgDAVkJsOdz4xnu8N1NoVDs2LFDu3/48OGsrKyPP/64Y8eOUqRii0ixR4gZAkR04ED9I7lffZXKy6ECAfkLsOdz4xlOoVC43N0U925KpXLJkiVS9GOLSLFHiNnJBWprKTmZ3Nz4TyZbt6aff3ZyD3TfiQTY87nxDHf69OnCwkKFQpGZmXm6bjt//nxtba1EFdkiEu0UwnZagdOnKTpa96WSESPoyhWnlUDHnVGAPZ8bz3DyM2OLyK+/6JGMBVasIJWKT28+PrRsGWk0Mu4rugaBRgTY87kZGe6HH354/PHH27Zte/r0aSKaM2fO+vXrG2lQ9KfYIqIPHwFCgBe4do3i4nRLt6goOnECLBBwRgH2fG5qhluwYEGrVq0++uijFi1anDx5koiWLVv25JNPSlGULSLFHiFmZxPYsYPat+fTm6srffgh1dQ4GwD6CwGdAHs+NzXDde3a9aeffiIi7bMFiOjo0aOBgYFSZGaLSLFHiNl5BKqqaNo03e8BOnemjAzn6Tp6CoFGBNjzuakZzsvLS/vhpD7D5efne3l5NdKg6E+xRUQfPgJ0XoHcXOrVS/fJ5MSJVFHhvBToOQS0Auz53NQM17VrV+1f3fQZbv78+b1795aiMltEij1CzLIX0Gho8WJSKvn01qoVSfMv4LIfJXTQAQLs+dzUDPftt9+Ghob++OOP3t7eq1at+uijj7Q7DuiQxU2yRSyuHhVAwMoCpaU0cqRu6fbUU3T+vJXrR3UQkK4Aez43NcMR0YoVKzp37qz92XdoaOjixYslisIWkWinELZcBVJTqUMHPr25udGnn5JaLdeOol8QaI4Aez43I8NpG6+srBTcoLI5QdVdU1JSEh8f7+vr6+fnN2HChIqm/7CQlpYWExOjVCp9fX379+9/8+7dZAsLCydMmNCpUycvL6/77rvvgw8+qK6urqu7yX+zRZq8DC9AwL4CNTX0j3+Qiwuf3u6/n/bts2/zaA0CUhBgz+dmZzjrdjk2NrZnz57p6em7d+/u3LlzXFxco/WnpaWpVKrk5OTs7Oy8vLzVq1dXVVUR0ebNm1955ZWUlJSTJ09u2LAhODj43XffbbQGw5NsEcOS2IeAowTOnKEnntB9Mjl2LF2/7qhA0C4ERC3Ans9NzXAXL14cPXp027ZtXV1dtbep1P7Tkq7n5ORwHJdZ9+iqzZs3KxSKc+fONayzT58+s2bNanhecOZf//pXWFiY4KT2sKqqqrxuKyoq4jiuHDembVQKJ0UgsHYt+fvz6c3Xl1asEEFACAECYhWwToaLjY198MEHFyxY8NNPP6032Czp9ZIlS/z9/fU11NTUuLq6rlu3Tn9Gu3Pp0iWO4+bPnx8VFRUcHBwdHb17925BGe3h+++//8gjjzT6UmJiInfvhgzXKBROOlbg1i2aNEm3dIuMxJ1KHDsaaF0CAtbJcD4+PgcPHrRud5OSkrp06WJYZ1BQ0IIFCwzPENHevXvvZLiAgIClS5dmZWVNnTrVw8MjPz9fUKygoEClUi1atEhwXnuINVyjLDgpKoHjx6lnT116mz6dbt8WVXQIBgJiFLBOhuvatWtWVpa5/Zs+ffq9C6f6o9zcXBMz3J49eziOmzFjhr71Hj16JCQk6A+JqLi4+P7773/11VcNTza1zxZp6iqch4BNBVasIG9vPr0FBdGWLTZtCpVDQD4C7Pnc1L/DpaSkDB48uLCw0CyYy5cv5zaxVVdXm/gp5alTpziOW758ub7pUaNGxcfH6w/PnTsXHh4+ZswYtWnfpGaL6KvFDgTsI1BZSRMm6JZuTz5Jjf0l2j6BoBUISE+APZ8byXD+/v4t6zYPDw8XFxcfH5+6E/y/LfHQftNk//792kpSUlIa/aaJRqMJCQkx/KZJr1699Eu64uLi8PDwl156yfTn1bFFLOkRroWAuQLZ2boHcysU/G8DJPvURXP7jfIQsI4Aez43kuG+M7ZZGGNsbGzv3r0zMjJSU1PDw8P1vxYoLi6OiIjIqLut7Ny5c1Uq1Zo1awoKCmbNmuXl5XXi7sNCiouLO3fuPGjQoOLi4gt1m9GQ2CJGL0cBCFhFQKOhJUuoRQt+9damDW3fbpVaUQkEnEuAPZ8byXBE9P3332t/fGYLtpKSkri4OB8fH5VKNX78eP0vvgsLCzmO27Fjh77R5OTkdu3aKZXKqKgo/Xcply1bVv+Xvbo9/SVN7bBFmroK5yFgRYGKCnr5Zd0nk4MH06VLVqwbVUHAiQTY87nxDOfi4mLFm5iIAZ4tIoYIEYO8BbKz6YEH+PTm6koff4wbccl7tNE72wqw53PjGU6hUCDD2XaIULszCSxfrntEQEgI7drlTD1HXyFgAwErZLjLly/bIDCHVckWcVhYaFjuArdu0V/+ovtk8qmn8Mmk3Mcb/bOLAHs+N2kN16NHj95NbHbpgpUbYYtYuTFUB4G7AidP0sMP8+lNoaDERHxnEm8LCFhHgD2fm5Th3nvvvX80sVknRvvWwhaxbyxozSkENmwgPz8+vQUGUkqKU3QZnYSAfQTY87lJGQ5/h7PPUKEV+QnU1NDf/qb7ZDIqis6elV8X0SMIOFLA0gyH71JEZ3lBAAAgAElEQVQ6cvTQtpQFzp+n/v116W3qVDLh2YVS7i1ih4AjBCzNcPgupSNGDW1KXmDXLv533BxHKhWtXSv57qADEBCngKUZ7vTp0ybe71Gc/W8YFVukYXmcgYBZAhoNffEFubnx6a1HD2rwGAyzKkNhCECAJcCez438HS45OfnmzZuM6tPT03/55RdGARG+xBYRYcAISUICN25QfLzuk8n4eLpxQ0KxI1QISE+APZ8byXBjxoxp1arVpEmTNm3apP9VXE1NzeHDh7/++uuoqKiOHTvu3LlTWipsEWn1BdGKSqCggF+0cRy/gJs3jzQaUUWHYCAgQwH2fG4kwxHRoUOHXnvtNX9/fxcXF3d3dx8fH5e72yOPPLJw4cJbt25JzowtIrnuIGCRCPz8s+4nAW3aUBNPoRdJpAgDAvIRYM/nxjOcVkKtVh88eHD9+vWrVq3aunXrlStXpCvEFpFuvxC5owRqa2nWLN0nk/360fnzjgoE7ULA6QTY87mpGU5ObGwROfUUfbGDQEkJxcbq0ttbb+EnAXYgRxMQqBdgz+dGMly5sa2+HenssUWk0w9E6niBI0fovvv49NaiBRk8hd7xgSECCDiJAHs+N5LhFAqF9q9uTf1TiohsESn2CDE7RGDtWvL25tNbWBgdOuSQENAoBJxdgD2fG8lwv9dt3333XZs2bRISEjbc3RISEtq2bfvdd99JUZctIsUeIWY7C6jV9P77uk8mn3qKSkrs3D6agwAEdALs+dxIhtMrDhw48L///a/+kIhWrlw5YMAAwzNS2WeLSKUXiNNRAmVl9Mc/6tLbu+9STY2jAkG7EIAAsedzUzNcixYt8u+9N8Px48dbtGghRWC2iBR7hJjtJpCXRxERfHrz9MQf3uymjoYg0KQAez43NcN16dJl2rRpho1MmzatS5cuhmekss8WkUovEKf9BX75hb/JJMdRu3aUmWn/9tEiBCAgFGDP56ZmuI0bN3p5eXXv3v3Vu1uPHj28vLw2btwobE0Kx2wRKfQAMdpbQKOhpCT+4aUcR088QRcv2jsAtAcBCDQqwJ7PTc1wRFRUVDRjxozn7m4zZ848K9lHXbFFGkXESWcWuHGDRo7U/eHt9dfxizdnfi+g76ITYM/nZmQ40fWsuQGxRZpbK66Tp8DZs9S7N5/e3N3pm2/k2Uf0CgLSFWDP52ZkuNLS0s8//1z7KeWcOXPKysokisIWkWinELYtBNLTqXVrPr0FBeFWk7YARp0QsFSAPZ+bmuEyMzMDAgJCQ0O1n1K2a9cuMDDwwIEDlkbniOvZIo6ICG2KUWD5cv4Lk9pnvBUWijFCxAQBCLDnc1Mz3BNPPPHKK6/U1P32p6amZty4cf3795eiL1tEij1CzNYVUKspIUH3h7dhw+j6detWj9ogAAGrCbDnc1MznJeXV25urmFQx44dw+/hDEGwLw+B69dp2DBdektIILVaHt1CLyAgTwHrZLjg4OCUlBRDoS1btgQHBxuekco+W0QqvUCcthAoLNQ9whQ/6LYFL+qEgNUF2PO5qWu4t956q127dj/++OPZu9uqVavatWs3ZcoUq4drhwrZInYIAE2IU2D3bv4bJRzHf7tk715xxoioIACBewTY87mpGa66uvrtt9/28PDQPmTA09Nz6tSpVVVV9zQlkQO2iEQ6gTCtLPD99/zvATiO/22AZH/qaWUTVAcB8Quw53NTM5y2n5WVlUfubpWVleLveVMRskWaugrn5SqgVtc/ofv55+nGDbl2FP2CgAwF2PO5eRlOe2eToqIiSTuxRSTdNQRvrsCtW/Tii/heiblsKA8BsQiw53NTM5xarf7nP/+pUqm0n1L6+fl9+OGHaml+z4wtIpZxQxy2F7h0iaKi+PTm5kZLlti+PbQAAQhYW4A9n5ua4RISEoKCghYsWHD47vb1118HBQXNnDnT2tHaoz62iD0iQBsiEDh2jH82N8eRvz9t3y6CgBACBCBgvgB7Pjc1w7Vt23bDhg2Gra9fvz4kJMTwjFT22SJS6QXitERg61by8+PT23330b2/87SkVlwLAQjYW4A9n5ua4Tw9PY8fP24Ye15enpeXl+EZqeyzRaTSC8TZbIFFi8jVlU9v/frRlSvNrgYXQgACjhdgz+emZrjIyMi33nrLsDdvvvlmnz59DM9IZZ8tIpVeIM5mCKjVNG0an9s4juLj6datZtSBSyAAAREJsOdzUzPc77//7u3t3bVr1wl3t65du/r4+OzatUtEHTU5FLaIydWgoMQEbt6k55/XpbfERNJoJBY/woUABBoKsOdzUzMcEZ07d27mzJkj7m7vv//+uXPnGjYmiTNsEUl0AUGaK3D1Kj3+OJ/e3N1p+XJzr0Z5CEBApALs+dyMDCfS/pkfFlvE/PpwhdgFTp6kLl349ObnRzt2iD1axAcBCJguwJ7PzchwpaWlKSkpy5cv/95gMz0O8ZRki4gnTkRiFYGMDN3dJtu3p+xsq1SJSiAAAbEIsOdzUzPczz//7Ovrq1Ao/Pz8/Ou2li1biqWX5sTBFjGnJpQVu8CGDdSiBb9669WLJPuxutiRER8EHCjAns9NzXDh4eFTpkyR9O0o9WPAFtEXw47UBb7+mlxc+PQ2ZAieYir1wUT8EGhcgD2fm5rhlErlyZMnG29BamfZIlLrDeJtRECtpr/9jc9tHEevvkq3bzdSBqcgAAEZCLDnc1Mz3HPPPbd69WoZcBARW0QefXTmXhjeTHn2bPwqwJnfC+i7/AXY87mRDLehblu8eHGHDh0SExPXrl1bd47/txT92CJS7BFi1guUldGAAfzSzc2Nvv9efxo7EICAPAXY87mRDKdgbi4uLlI0Y4tIsUeIWStw7hw99BCf3nx9aetWqEAAAvIXYM/nRjKcLHnYIrLssjN0Ki+POnbk01vr1pSV5Qw9Rh8hAAEjf3VycIYrKSmJj4/39fX18/ObMGFCRUVFUyOWlpYWExOjVCp9fX379+9/8+ZNw5JVVVU9e/bkOO7gwYOG5xvdR4ZrlEXSJ9PTKTCQT2+dO5NcvhEl6QFB8BCwkwB7PjeS4ebNm3fr7u1p5zWxWdiJ2NjYnj17pqen7969u3PnznFxcY1WmJaWplKpkpOTs7Oz8/LyVq9eXVVVZVjy7bffHjp0KDKcoYnz7G/aREoln94efZQuXXKefqOnEICAZWu4Tp06Xb16lYg6NbaFhYVZApyTk8NxXGZmpraSzZs3KxSKRm932adPn1mzZjXV1qZNmx544IFjx44hwzVFJOPz33+vexTOkCHU9EcAMgZA1yDg1AIWreFsKrdkyRJ/f399EzU1Na6uruvWrdOf0e5cunSJ47j58+dHRUUFBwdHR0fv3r1bX+bixYuhoaGZmZmFhYWMDFdVVVVetxUVFXEcV15erq8EO1IU0Gjo00/5pRvH0ejRVF0txU4gZghAwCIB8Wa4pKSkLl26GHYuKChowYIFhmeIaO/evXcyXEBAwNKlS7OysqZOnerh4ZGfn09EGo0mNjZ29uzZRMTOcImJidy9GzKcwFlah2o1TZ2qS2/vvUdqtbTCR7QQgIB1BCzKcO8Y29gxTp8+/d60Un+Um5trYobbs2cPx3EzZszQt9WjR4+EhAQimjdvXr9+/Wpra41mOKzh9Hoy2Kmu5p9fql29/fvfMugQugABCDRTwKIM9yRzi4mJYQd1+fLl3Ca26upqEz+lPHXqFMdxyw0e6jVq1Kj4+HgiGj58uIuLi2vdxnGcq6vr2LFj2VGxRdjX4lWHC1RW0tChfHpzc6MVKxweDgKAAAQcKcCez418l9KmgWu/abJ//35tKykpKY1+00Sj0YSEhBh+06RXr17aJd2ZM2eO1m0pKSkcx61du7aoqIgdNluEfS1edaxAaSn168entxYtaPNmx8aC1iEAAccLsOdz8zJcQUHBli1btL9F02g0lncuNja2d+/eGRkZqamp4eHh+l8LFBcXR0REZGRkaJuYO3euSqVas2ZNQUHBrFmzvLy8Tpw4IWid/Xc4w8JsEcOS2BeVwIUL1LMnn978/Sk1VVShIRgIQMAxAuz53NQMd/Xq1YEDByoUChcXF+1DBsaPH//Xv/7Vwj6VlJTExcX5+PioVKrx48frf/GtTVc7DJ7HnJyc3K5dO6VSGRUVZfhdSn0AyHB6ClnunDpF99/Pp7fWrenwYVl2EZ2CAATMFrBOhhszZsyQIUOKiop8fHy0GW7Lli0PPvig2eGI4AK2iAgCRAhCgexsCgnh01tYGDVYvQsL4xgCEHAeAfZ8buoarnXr1ocOHSIifYY7efKkt7e3FB3ZIlLskbxjTk+ngAA+vXXrhud0y3uo0TsImC3Ans9NzXA+Pj7an6DpM1xmZmZAQIDZ4YjgAraICAJECPUCW7eStzef3vr2pZKS+vPYgwAEIGD0eZ+mZrihQ4dqv83o4+Nz6tQptVo9cuTI559/XorEyHBSGbW1a8nDg09vTz+NO3JJZdAQJwTsKsCez03NcEePHg0ODo6NjfXw8HjhhRe6du3aunXrhl9otGvPmtsYW6S5teI6Kwt8/z25uPDp7fnn6d77bFu5IVQHAQhIV4A9n5ua4YiorKzso48+Gjly5NChQ99///3z589LFIUtItFOySzsBQv43MZxNGEC3b1ljcz6h+5AAALWEWDP56ZmuO3btzcM56uvvmp4Uvxn2CLij1/2EX72mS69vf02bjgp+9FGByFgkQB7Pjc1w/n7++tvPqIN54svvvD19bUoNAddzBZxUFBolhe4cxeBxERdeps5kz/EBgEIQIAhwJ7PTc1w3377bVBQUG5urralzz//XKVS7dq1i9GwaF9ii4g2bNkHptHQe+/p0ltSkuy7iw5CAAJWEGDP56ZmOCL69NNPQ0NDCwsLP/nkE5VKlSrZ+yaxRaxAjirMF1CradIkXXr74gvzr8cVEICAUwqw53MzMhwR/e1vfwsMDPT399+7d690Mdki0u2XdCOvqaGxY/n0plDQ4sXS7QcihwAE7C3Ans+NZLh5Dbb27du//PLL+tP27o012mOLWKMF1GGGQHU1vfACn95cXem//zXjQhSFAAQgwJ7PjWS4TswtLCxMir5sESn2SLox37pFf/gDn948PGj9eun2A5FDAAKOEWDP50YynGNCtnGrbBEbN47q6wUqK/mblWgf9paSUn8eexCAAARMFGDP58hwJjKimJUFbtygmBg+vXl70++/W7lyVAcBCDiJgEUZ7p133rlx48adu1u+08QmRUS2iBR7JLmYKyooOppPb76+eJap5EYPAUNARALs+dzIGu7JJ58sLS0loicb22JiYkTUUZNDYYuYXA0KNlOgvJz69ePTm0pFUv5ObjO7j8sgAAErCrDncyMZzopxiKcqtoh44pRlJGVl/HNwOI78/WnfPll2EZ2CAATsJ8Cez5Hh7DcSaOnaNXr0UT69BQTQgQPwgAAEIGCpgEUZ7jljm6XROeJ6togjInKKNq9epd69+fTWqhXdfVy8U/QanYQABGwqwJ7PjazhXjG22TR0G1XOFrFRo05e7eXL9NBDfHoLDqajR50cA92HAASsJsCez41kOKtFIaaK2CJiilQmsVy6RN268emtTRvKyZFJp9ANCEBADALs+dzsDJecnKz9dqUY+ta8GNgizasTVzUloE9vISGUl9dUKZyHAAQg0BwB9nxudobz9fU9efJkcwIRzTVsEdGEKYdALl+mHj341VtICOXny6FH6AMEICAqAfZ8bnaG8/HxQYYT1QCLNpgrV3R/e2vbFulNtKOEwCAgbQFkOOH4sUWEpXHcLIGrV6lnT93f3vDhZLMIcREEIGBcgD2fm72GO3v2bG1trfFmRVyCLSLiwCUTWkmJ7ocBrVtT3WPhJRM8AoUABCQkwJ7Pzc5wEup5U6GyRZq6CudNFLh2jR5+WPfDgGPHTLwIxSAAAQg0R4A9n5ua4fz9/VveuwUEBISEhERHRy9durQ5cTnuGraI4+KSQ8ulpbq7lgQFUXa2HHqEPkAAAmIWYM/npma4OXPmBAYGjh49ev7dbfTo0a1atUpKSnrttdc8PT0XLVokZgJBbGwRQWEcmi5QVkaPPcav3lq1oiNHTL8OJSEAAQg0U4A9n5ua4UaMGLFw4ULDEP7zn/+MGDGCiObPn9+9e3fDl0S+zxYRefCiDa+8nPr04dNbYCAdPizaMBEYBCAgKwH2fG5qhvP29i4oKDCEKSgo8Pb2JqITJ04olUrDl0S+zxYRefDiDK+igh5/nE9vAQF08KA4Y0RUEICADAXY87mpGa59+/Zz5swx5JkzZ0779u2J6PDhw61btzZ8SeT7bBGRBy/C8G7e1D2t29+fsrJEGCBCggAEZCvAns9NzXCLFi1ydXX905/+NPvuNmzYMDc3t8WLFxPR559/PmrUKAn5sUUk1BExhFpVRbGx/OrN15cyMsQQEWKAAAScSIA9n5ua4YgoNTX1pZde6n13e+mll/bs2SNRRbaIRDvlkLBv36Znn+XTm1JJu3Y5JAQ0CgEIOLUAez43I8PJRpEtIptu2rojtbX00kt8evP0pK1bbd0a6ocABCDQiAB7Pjcjw9XW1q5du1b7KeW6deuke2cTtkgjhDjVQECtplde4dObuzv98kuDl3ECAhCAgF0E2PO5qRmuoKAgPDxcqVRqP6VUKpUREREnTpywSxes3AhbxMqNybE6jYYmTeLTm6srrV0rxx6iTxCAgEQE2PO5qRlu6NChsbGxJSUl2l5fvXo1Njb2mWeekQjCPWGyRe4pioMGAhoNvfMOn94UClq5ssHLOAEBCEDAjgLs+dzUDKdUKo/ce5uKQ4cOaX8PZ8e+WKcptoh12pBvLe+/z6c3jqMlS+TbSfQMAhCQiAB7Pjc1w7Vs2VLw5cnU1NSWLVtKBOGeMNki9xTFwb0CSUm69PbVV/e+gCMIQAACjhBgz+emZrgxY8Z069YtPT1dc3fbu3dv9+7dx40b54geWdomW8TS2uV7/Zdf6tLbZ5/Jt5PoGQQgICkB9nxuaoYrLS0dNmyYQqHwuLspFIpnn322tLRUUhS6YNkiUuyRHWJevlyX3j74wA6toQkIQAACJgmw53NTM5y2qYKCgp/vboJ7VJoUiGgKsUVEE6aIAlm/nv/aJMfR22+TRiOiwBAKBCDg5ALs+dxIhnvH2CZFXLaIFHtk05i3b+d/081xNG4cqdU2bQqVQwACEDBPgD2fG8lwTzK3mJgY82IRR2m2iDhiFEsUGRnk48Ont2efpZoasUSFOCAAAQhoBdjzuZEMJ0tEtogsu9y8Th09yj8Nh+No0CC6dat5deAqCEAAAjYUYM/nyHA2pJd01SdPUtu2fHrr04cqKiTdFQQPAQjIVgAZTji0bBFhaac8PneOwsL49Na9O9Xdx8YpIdBpCEBA3ALs+dzBa7iSkpL4+HhfX18/P78JEyZUNL1YSEtLi4mJUSqVvr6+/fv3v3nzpp79l19+iYyM9PLy8vf3Hz58uP58Uztskaaucp7zV69St258erv/fjp/3nn6jZ5CAALSE2DP5w7OcLGxsT179kxPT9+9e3fnzp3j4uIaBU5LS1OpVMnJydnZ2Xl5eatXr66qqtKWXLt2bcuWLRcuXHj8+PFjx46tXr260RoMT7JFDEs64X5FBUVG8uktJIROnXJCAHQZAhCQkgB7PndkhsvJyeE4LjMzU8u5efNmhUJx7ty5hrp9+vSZNWtWw/M1NTWhoaHaR403fLWpM2yRpq5yhvPV1TR4MJ/eAgLo2DFn6DH6CAEISFuAPZ87MsMtWbLE399fr1tTU+Pq6rpu3Tr9Ge3OpUuXOI6bP39+VFRUcHBwdHT07t27tS9lZGRwHLd06dJevXq1adMmNjb26NGjgsu1h1VVVeV1W1FREcdx5eXljZZ02pNqNcXF8elNqaT0dKdlQMchAAEpCYg3wyUlJXXp0sXQMigoaMGCBYZniGjv3r13MlxAQMDSpUuzsrKmTp3q4eGRn59PRKtWrbqT4Tp06LB27dr9+/fHxcUFBgbqH/FjWE9iYiJ374YMZ+ij0dCUKXx6c3OjLVsMX8E+BCAAAfEKODLDTZ8+/d60Un+Um5trYobbs2cPx3EzZszQG/fo0SMhIYGIVq5cyXHcN998o32pqqqqVatW//nPf/Ql9TtYw+kpGt35+GM+vXEcrVjR6Os4CQEIQECMAo7McJcvX85tYquurjbxU8pTp05xHLd8+XK97qhRo+Lj44lo+/btHMfpP7QkosjIyJkzZ+pLNrrDFmn0EnmfXLxYl97mzpV3R9E7CEBAbgLs+dyRf4fTftNk//79WvKUlJRGv2mi0WhCQkIMv2nSq1cv7ZKuvLzc09NT/02T27dvBwcH65d0TY0kW6Spq+R6fsMGcnHhM9z06XLtIvoFAQjIVoA9nzsywxFRbGxs7969MzIyUlNTw8PD9b8WKC4ujoiIyMjI0A7L3LlzVSrVmjVrCgoKZs2a5eXldeLECe1LU6ZMCQ0NTUlJycvLe/XVV4ODg69du8YeTLYI+1qZvbprF3l58elt/Hg8NEBmY4vuQMApBNjzuYMzXElJSVxcnI+Pj0qlGj9+vP4X34WFhRzH7dixQz9EycnJ7dq1UyqVUVFRhh9L3r59+9133w0ODvb19X3qqaeys7P1lzS1wxZp6ir5nT9yhPz9+fT2pz/hrsryG170CAJOIcCezx2c4RwyAmwRh4Rk/0ZPn+Z/081x1K8fVVbav320CAEIQMAKAuz5HBnOCsSSq+LKFerShU9v3bqRsc90Jdc5BAwBCDiRADKccLDZIsLSsjuurKS+ffn01qEDFRfLrnvoEAQg4EwC7Pkcazhnei8Q1dbS8OF8emvZknJynKvv6C0EICA/AWQ44ZiyRYSlZXSs0dAbb/DpzdOT6m58JqPuoSsQgIDzCbDnc6zhnOgd8emnfHpTKGjtWifqNboKAQjIWAAZTji4bBFhabkcr1zJpzeOoy++kEuX0A8IQMDpBdjzOdZwTvEG2baN3N359PbXvzpFf9FJCEDASQSQ4YQDzRYRlpb+8ZEjpFLx6W3UKFKrpd8f9AACEIBAnQB7Pscars5Jpv8uKqLQUD69RUfTrVsy7SS6BQEIOKsAMpxw5NkiwtJSPi4tpe7d+fTWtSt+2S3lgUTsEIBAEwLs+RxruCbYpH+6qoqefJJPb23b0pkz0u8PegABCECggQAynJCELSIsLc1jjYZGj+bTm68vHTwozT4gaghAAALGBNjzOdZwxvyk+fo//sGnN1dXSkmRZgcQNQQgAAETBJDhhEhsEWFpCR4vX86nN46jRYskGD1ChgAEIGCyAHs+xxrOZEiJFNy5U/fTNzyzWyIjhjAhAIHmCyDDCe3YIsLSkjo+fpy/pTLH0Qsv4Kdvkho5BAsBCDRLgD2fYw3XLFRRXnTlCnXuzKe3Pn3o5k1RhoigIAABCFhVABlOyMkWEZaWyPGtW/zTujmOOnWiixclEjTChAAEIGCZAHs+xxrOMl1xXK3RUFwcn978/OjYMXHEhCggAAEI2F4AGU5ozBYRlpbC8d//zqc3Nzf67TcphIsYIQABCFhJgD2fYw1nJWbHVfPdd3x64zhassRxQaBlCEAAAo4QQIYTqrNFhKXFfbxjh+63ATNmiDtQRAcBCEDABgLs+RxrOBuQ26vK/HzdbwPwWBx7kaMdCEBAXALIcMLxYIsIS4v1uLSUIiL4DycjI/HbALEOEuKCAARsLMCez7GGszG/baqvqaHBg/n01q4dnT9vmzZQKwQgAAHRCyDDCYeILSIsLcrjt97i05tSSVlZoowPQUEAAhCwiwB7Pscazi6DYNVGFi7k0xvH0f/9n1XrRWUQgAAEpCaADCccMbaIsLTIjrdt45+Jc+fXbx99JLLIEA4EIAABuwuw53Os4ew+IBY0qP/yZHw8aTQWVIRLIQABCMhCABlOOIxsEWFp0RzrvzzZpw/duiWasBAIBCAAAccJsOdzrOEcNzLmtFxTQ08/zX842b49XbhgzpUoCwEIQEC+AshwwrFliwhLi+P4zTf59KZU0sGD4ggIUUAAAhAQgQB7PscaTgRDZCwE/Zcn160zVhSvQwACEHAmAWQ44WizRYSlHX38++/8QwM4jj7+2NGhoH0IQAACIhNgz+dYw4lsuO4N5/RpatWKT29xcfjy5L00OIIABCBAhAwnfBewRYSlHXdcWUm9evHp7eGHqbLScXGgZQhAAAJiFWDP51jDiXTcNBp68UU+vQUF0ZkzIg0SYUEAAhBwrAAynNCfLSIs7aDj5GQ+vbm50a5dDooAzUIAAhAQvQB7PscaTowDuHEjKRR8hlu4UIzhISYIQAACIhFAhhMOBFtEWNrux3l5pFLx6e0vf7F722gQAhCAgKQE2PM51nDiGsyyMt1zTZ94gqqrxRUbooEABCAgNgFkOOGIsEWEpe14XFtLf/gDv3pr144uXrRjw2gKAhCAgDQF2PM51nAiGtWZM/n05uVF+/eLKCqEAgEIQEC0AshwwqFhiwhL2+t49Wo+vXEcrVxprybRDgQgAAGJC7Dnc6zhRDG8hw/zd1XmOJo2TRTxIAgIQAACkhBAhhMOE1tEWNr2x9eu0f338+lt8GCqrbV9e2gBAhCAgFwE2PM51nAOHme1mp55hk9vnTrR1asODgbNQwACEJCWADKccLzYIsLSNj7+xz/49OblRVlZNm4J1UMAAhCQnQB7PnfwGq6kpCQ+Pt7X19fPz2/ChAkVFRVN+aelpcXExCiVSl9f3/79+9+8eVNb8vjx48OGDQsMDPT19e3Xr9/27dubqkF/ni2iL2aHnf/3//j0xnH0/fd2aA1NQAACEJCbAHs+d3CGi42N7dmzZ3p6+u7duzt37hwXF9cof1pamkqlSk5Ozs7OzsvLW716dVVVlbZkeHj4M888c9ctudgAABNZSURBVPjw4fz8/DfeeEOpVF64cKHRSvQn2SL6YrbeKSggPz8+vU2ebOumUD8EIAABeQqw53NHZricnByO4zIzM7XwmzdvVigU586dazgOffr0mTVrVsPzV65c4ThuV93Nia9fv85x3NatWxuWrKqqKq/bioqKOI4rLy9vWMxuZ27coB49+PT2+OO4d4nd1NEQBCAgNwHxZrglS5b4+/vrvWtqalxdXdetW6c/o925dOkSx3Hz58+PiooKDg6Ojo7evXu39iWNRhMREfHaa6/duHGjpqbms88+Cw4OvnbtmqCGO4eJiYncvZsDM5xGwz/R9M6NlVu3psYSesPwcQYCEIAABBoREG+GS0pK6tKli2HIQUFBCxYsMDxDRHv37r2T4QICApYuXZqVlTV16lQPD4/8/HxtsaKiokceeUShULi6urZt2zariS9siGoN98UXfHrDk3EEA41DCEAAAuYKODLDTZ8+/d6FU/1Rbm6uiRluz549HMfNmDFD3/MePXokJCQQkUajGTZs2NChQ1NTUw8cODBp0qTQ0NDz58/rSza6wxZp9BIrnvz9d3J15TPcvHlWrBVVQQACEHBGAfZ8btu/w12+fDm3ia26utrETylPnTrFcdzy5cv1ozdq1Kj4+Hgi+u2331xcXAw/b+zcuXNycrK+ZKM7bJFGL7HWyeJiCg7m09vLL5NGY61aUQ8EIAABJxVgz+e2zXBscu03TfbX3WY4JSWl0W+aaDSakJAQw2+a9OrVS7uk+/nnn11cXAx/Y9ClS5ekpCR2u2wR9rWWvFpdTX378untoYeostKSmnAtBCAAAQjwAuz53JEZjohiY2N79+6dkZGRmpoaHh6u/7VAcXFxRERERkaGdgznzp2rUqnWrFlTUFAwa9YsLy+vEydOENGVK1cCAwNHjBhx6NCh48ePv/fee+7u7ocOHWKPPFuEfa0lr771Fp/e/P3pbuyW1IRrIQABCECAF2DP5w7OcCUlJXFxcT4+PiqVavz48frVWGFhIcdxO3bs0I9hcnJyu3btlEplVFSU/ruURJSZmTl48OCAgABfX9++fftu2rRJf0lTO2yRpq6y8PyPP/LpjePol18srAmXQwACEICAToA9nzs4wzlklNgitggpN5d8fPj0NnOmLapHnRCAAAScVIA9nyPD2fxtceMGdevGp7eYGKqpsXlzaAACEICA8wggwwnHmi0iLG3ZsUZDo0fz6a1tWzJ2NzHLWsLVEIAABJxPgD2fYw1n23fEN9/w6c3VlXbutG1DqB0CEICAEwogwwkHnS0iLG3B8YED5OnJZ7hPP7WgFlwKAQhAAAJNCLDnc6zhmmCz+PS1axQWxqe3YcNIrba4OlQAAQhAAAINBJDhhCRsEWHpZh1rNHxiu/Pgt7AwauxG0M2qFBdBAAIQgMC9Auz5HGu4e7WsdPSvf/HpzdOTDhywUo2oBgIQgAAEGgggwwlJ2CLC0uYf79ypu7fyN9+YfzGugAAEIAABkwXY8znWcCZDmlbw4kX+hwEcR2PG4N7KppGhFAQgAIHmCiDDCeXYIsLS5hzX1tKgQXx669aNbtww50qUhQAEIAAB8wXY8znWcOaLNn3F7Nl8elMqKSen6UJ4BQIQgAAErCSADCeEZIsIS5t8vHMnubjwGe77702+BgUhAAEIQMACAfZ8jjWcBbQGl16+TCEhfHobN87gLHYhAAEIQMCWAshwQl22iLC0CcdqNQ0dyqe3Bx7An99M8EIRCEAAAlYSYM/nWMNZgVn76zcvLzpyxAq1oQoIQAACEDBRABlOCMUWEZY2dpyWRm5u/AJu0SJjRfE6BCAAAQhYVYA9n2MNZxF2SQl16MCnt5dewq/fLJLExRCAAASaIYAMJ0RjiwhLN32s0dDw4Xx669yZysubLodXIAABCEDANgLs+RxruOarz5vHpzcPD9x8svmGuBICEICAJQLIcEI9toiwdBPHmZnk7s5nuC+/bKIETkMAAhCAgI0F2PM51nDN4S8ro/vu49PbiBH481tzAHENBCAAAasIIMMJGdkiwtINjjUaGjWKT2+dOlFpaYOXcQICEIAABOwlwJ7PsYYzexxu36bXXuN/IZCebva1uAACEIAABKwogAwnxGSLCEs3cZyX18QLOA0BCEAAAvYSYM/nWMPZaxzQDgQgAAEIWFsAGU4oyhYRlsYxBCAAAQiIVYA9n2MNJ9ZxQ1wQgAAEIGBMABlOKMQWEZbGMQQgAAEIiFWAPZ9jDSfWcUNcEIAABCBgTAAZTijEFhGWxjEEIAABCIhVgD2fYw0n1nFDXBCAAAQgYEwAGU4oxBYRlsYxBCAAAQiIVYA9n2MNJ9ZxQ1wQgAAEIGBMABlOKMQWEZbGMQQgAAEIiFWAPZ9jDSfWcUNcEIAABCBgTAAZTijEFhGWxjEEIAABCIhVgD2fYw0n1nFDXBCAAAQgYEwAGU4oxBYRlsYxBCAAAQiIVYA9nzvjGq6srIzjuKKionJsEIAABCAgZYGioiKO48rKyhpNwc6Y4bQiHDYIQAACEJCFQFFRETKcTkCtVhcVFZWVlTX7/7hocyRWgYaAMDHU0O7DBCYNBRqewfvEEpOysrKioiK1Wo0M16hAc06yP/ltTo3SvwYmDccQJjBpKNDwDN4ntjNxxk8pG2qaewbvyIZiMIFJQ4GGZ/A+gUlDgYZnrPU+QYZraGv8jLX0jbcknRIwaThWMIFJQ4GGZ/A+sZ0JMlxDW+NnqqqqEhMT7/zTeFGnKQGThkMNE5g0FGh4Bu8T25kgwzW0xRkIQAACEJCDADKcHEYRfYAABCAAgYYCyHANTXAGAhCAAATkIIAMJ4dRRB8gAAEIQKChADJcQxOcgQAEIAABOQggwzVnFL/66quOHTt6enpGRkZmZGQ0pwqJX7Nz584//vGPbdu25Tjup59+0vdGo9H8/e9/b9OmjZeX16BBg/Lz8/UvyX7n448/fvTRR318fIKCgoYPH56Xl6fv8q1bt954442AgABvb+8RI0ZcvHhR/5K8dxYsWNCjRw/fu1vfvn03bdqk7a/TghgOd3JyMsdxU6ZMgUliYqLhvcMiIiKsZYIMZ/iWM2n/xx9/9PDwWLp06bFjxyZOnOjv73/p0iWTrpRRoU2bNr3//vvr1q0TZLhPPvnEz89v/fr1hw8fHjZsWFhY2K1bt2TUb1ZXhgwZsmzZsuzs7EOHDj3zzDMdOnS4ceOG9oLXX3+9ffv227Zt279/f9++fR9//HFWRTJ67eeff964cWN+fv7x48dnzpzp7u6enZ1NRE4Loh/bffv2derU6aGHHtJnOGc2SUxM7Nat24W67cqVK9b6DwcZTv+WM3UnMjJy8uTJ2tJqtTokJCQ5OdnUi2VXzjDDaTSaNm3afPbZZ9pelpWVeXp6rlq1SnadNt6hy5cvcxy3c+dOIiorK3N3d1+zZo32stzcXI7j9u7da7wW2ZVo2bLl4sWLAVJRUREeHr5169YBAwZoM5yTmyQmJvbs2VPwfreKCTKcQNXIYXV1taurq+HncmPHjh02bJiRy+T7smGGO3nyJMdxBw8e1Hc3Ojr67bff1h86z05BQQHHcUePHiWibdu23fkwqrS0VN/9Dh06zJkzR3/oDDu1tbWrVq3y8PA4duwYQMaOHTt16lQi0mc4JzdJTExUKpVt27YNCwuLj48/c+aMtf7DQYYzb3o5d+4cx3FpaWn6y6ZNmxYZGak/dLYdwwy3Z88ejuPOnz+vRxg5cuSoUaP0h06yo1ar//CHP/Tr10/b35UrV3p4eBj2/bHHHvvb3/5meEbG+0eOHPH29nZ1dfXz89u4cSMROTnIqlWrunfvrv30Xp/hnNxk06ZN//vf/w4fPrxly5aoqKgOHTpcv37dKibIcObNLchwAi9kOAGI9o9MHTt21D+wyir/oTZsRSpnqqurCwoK9u/fn5CQ0KpVq2PHjjkzyNmzZ4ODgw8fPqwdPmS4hm/j0tJSlUq1ePFiq7xPkOEaCrPO4FNKgY5hhsOnlEQ0efLkdu3anTp1Sg/l5B9A6R2IaNCgQX/+85+dGeSnn37iOM61buM4TqFQuLq6/vbbb/goW/9WefTRRxMSEqzyPkGG06uauhMZGfnmm29qS6vV6tDQUHzTRKuh/abJ559/rj0sLy93qm+aaDSayZMnh4SECH4jof2D+dq1a7UseXl5TvtNk5iYmHHjxjkzyPXr148abI8++ujo0aOPHj3qzCba/y70/6yoqGjZsuW8efOsYoIMp4c1defHH3/09PT87rvvcnJy/vznP/v7+zvPz5v0RhUVFQfvbhzHzZkz5+DBg9o/Dn/yySf+/v4bNmw4cuTI8OHDnerXApMmTfLz8/v999/rvvN84ebNm1qx119/vUOHDtu3b9+/f3/U3U0vKe+dhISEnTt3FhYWHjlyJCEhQaFQ/Prrr9oPcp0TRDDc+k8pndzk3Xff/f333wsLC/fs2fPUU0+1atXq8uXLVjFBhhO85Uw6/PLLLzt06ODh4REZGZmenm7SNfIqtGPHDsNfaHIcN27cOCLS/uK7devWnp6egwYNOn78uLz6zeqNAITjuGXLlmkv0P7AuWXLlkql8rnnnrtw4QKrIhm9NmHChI4dO3p4eAQFBQ0aNEib3ojIaUEEY2uY4ZzZ5MUXX2zbtq2Hh0doaOiLL7544sQJa/2HgwwneMvhEAIQgAAEZCKADCeTgUQ3IAABCEBAIIAMJwDBIQQgAAEIyEQAGU4mA4luQAACEICAQAAZTgCCQwhAAAIQkIkAMpxMBhLdgAAEIAABgQAynAAEhxCAAAQgIBMBZDiZDCS6AQEIQAACAgFkOAEIDiEAAQhAQCYCyHAyGUh0AwJGBTp27Dh37lyjxVAAArIRQIaTzVCiI9ITGDdu3PDhww2fhGnFPixbtszPz8+wwsuXL1dWVhqewT4E5C2ADCfv8UXvRC1gYYarrq5mdK9hhmMUxksQkKUAMpwshxWdkoaANsONGzfO8K7NhYWFRHT06NHY2Fhvb+/g4ODRo0dfuXJF26UBAwZMnjx5ypQpgYGBTz75JBH9+9//7t69u1KpbNeu3aRJkyoqKu4sCgW3xk5MTCQiw08pz5w5M2zYMG9vb19f35EjR+qfj5GYmNizZ88ffvihY8eOKpXqxRdfvH79urbpNWvWdO/e3cvLKyAgYNCgQTdu3JCGMqJ0YgFkOCcefHTd0QLaDFdWVhYVFTVx4kTtY3dqa2tLS0uDgoJmzJiRm5ublZX19NNPx8TEaIMdMGCAj4/PtGnT8u5uRDR37tzt27cXFhZu27YtIiJi0qRJRFRdXf3FF1+oVCptndq0p89warW6V69eTzzxxP79+9PT0x955JEBAwZo609MTPTx8RkxYsTRo0d37drVpk2bmTNnEtH58+fd3NzmzJmjfRTO119/ra3T0YRoHwIsAWQ4lg5eg4BNBZr6lHL27NmDBw/WN11UVMRxnPZRRAMGDOjdu7f+JcHOmjVrAgMDtScbfkqpz3C//vqrq6vr2bNntSWPHTvGcdy+ffvuHCYmJiqVSv26bdq0aX369CGiAwcOcBx3+vRpQYs4hICYBZDhxDw6iE3mAk1luBdeeMHd3d3bYOM4btOmTdrvpLz22muGLlu3bh04cGBISIiPj4+XlxfHcdqvkzAy3Lx58zp16mRYib+///fff3/nTGJi4oMPPqh/ac6cOWFhYURUW1s7aNAgX1/fF154YdGiRdeuXdOXwQ4ERCuADCfaoUFg8hdoKsPFxsaOGDGi4N5N+3cvw2dmElFhYaGnp+fUqVP37t17/PjxJUuWcBxXWlp6569uzc5wPXv21NPPnTu3Y8eO2kONRpOamvrBBx/06NEjKCjo1KlT+mLYgYA4BZDhxDkuiMopBPQZ7umnn37zzTf1fZ45c2ZERERNTY3+jH5HkOHWrl3r7u6uVqu1BWbPnq3PcCtXrvTx8dFfaPhNk0Y/pczMzLxTWPtNE/1VhhlOf7K2tjY0NPTf//63/gx2ICBOAWQ4cY4LonIKAX2Gmzhx4mOPPVZYWHjlyhW1Wn3u3LmgoKAXXnhh3759J06c2LJlyyuvvFJbW9vwl3OHDh3iOO6LL744efLkDz/8EBoaqs9we/bs4Tjut99+u3LlivZzS/3f4TQaTa9evfr373/gwIGMjAzBN00aXcOlp6cnJSVlZmaeOXPmf//7n4eHh/ZTU6cYJ3RSsgLIcJIdOgQufQF9hjt+/Hjfvn1btGjBcZz21wL5+fnPPfecv79/ixYtHnjggalTp2o0moYZjojmzJnTtm3bFi1aDBky5IcfftBnOCJ6/fXXAwMDOY4z99cCelr9Gi4nJ2fIkCFBQUGenp5dunT58ssv9WWwAwHRCiDDiXZoEBgEIAABCFgkgAxnER8uhgAEIAAB0Qogw4l2aBAYBCAAAQhYJIAMZxEfLoYABCAAAdEKIMOJdmgQGAQgAAEIWCSADGcRHy6GAAQgAAHRCiDDiXZoEBgEIAABCFgkgAxnER8uhgAEIAAB0Qogw4l2aBAYBCAAAQhYJIAMZxEfLoYABCAAAdEKIMOJdmgQGAQgAAEIWCSADGcRHy6GAAQgAAHRCvx/eBILrURiXdwAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "e87caf56",
   "metadata": {
    "id": "4tHm2tj5E3-P"
   },
   "source": [
    "-------------------------------------------------------\n",
    "\n",
    "\n",
    "**WRITE YOUR ANSWER HERE:**\n",
    "\n",
    "1. Using the gradient ascent rule, we are looking for a local maximum. This is because gradient ascent aims to maximize the likelihood function (or equivalently, the log-likelihood) by adjusting the model parameters to increase the probability of correctly classifying the training examples.\n",
    "2. Yes, we could have used gradient descent instead of gradient ascent for the proposed problem, but the objective would have been slightly different. In gradient descent, we minimize the negative log-likelihood (which is the opposite of the log-likelihood). Essentially, by applying gradient descent, we would be looking to find the parameters that minimize the cost function, by applying gradient ascend instead, we are looking for the parameters that maximize the likelihood. Both approaches (gradient ascent and gradient descent) are based on the same principle of adjusting the parameters in the direction of the gradient, but while gradient ascent maximizes the log-likelihood, gradient descent minimizes the negative log-likelihood.\n",
    "3. The learning rate α controls how large each step is during the update of the parameters. A larger learning rate means bigger steps, while a smaller learning rate means smaller steps. If α is too large, the algorithm might overshoot the optimal point and fail to converge, possibly causing oscillations or divergence of the parameter updates. If α is too small, the algorithm will converge very slowly, requiring more iterations to reach the optimal solution. A good value of α is a value that is large enough to make progress quickly but not so large that it causes instability.\n",
    "\n",
    "    By playing with the parameter, I can observe that if it's too small (e.g. 0.01), the algorithm fails to converge in 50 iterations. If it's too large (e.g. 15), we can see that the log-likelihood starts oscillating instead of converging. We can see that if we chose a value that does not let the algorithm converge, the decision boundary will not be the optimal one. For example, can make it extremely evident by chosing a very small value, like 0.01.\n",
    "4. By skipping the normalization step, the algorithm failed to converge to the optimal value. By playing with the parameters, the log-likelihood either started oscillating or converged to a suboptimal value. This is because the features are not on the same scale, and the algorithm has a hard time finding the optimal parameters to maximize the likelihood function. When the features are on different scales, the gradient updates for features with larger values are much larger, leading to uneven parameter updates. This can cause the algorithm to take large steps in some directions and much smaller steps in others, which may prevent it from reaching the optimal parameters.\n",
    "\n",
    "Some screenshots or our results:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "log-likelihood plot for alpha=0.05 and 50 iterations (fails to converge in time. It will converge but it will take much longer)\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "log-likelihood plot for alpha=15. Fails to converge and starts oscillating (for the reasons we said before)\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ad46b",
   "metadata": {
    "id": "vbdZNYCl3uGD",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2: **Polynomial Expansion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022266f7",
   "metadata": {
    "id": "TrcB4LXw3uGD",
    "tags": []
   },
   "source": [
    "### **2.1: Polynomial features for logistic regression** \n",
    "\n",
    "Define new features e.g., of 2nd and 3rd degree, and learn a logistic regression classifier by using the new features and the gradient ascent optimization algorithm defined in Question 1.\n",
    "\n",
    "In particular, consider a polynomial boundary with equation:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x_1, x_2) = c_0 + c_1 x_1 + c_2 x_2 + c_3 x_1^2 + c_4 x_2^2 + c_5 x_1 x_2 + c_6 x_1^3 + c_7 x_2^3 + c_8 x_1^2 x_2 + c_9 x_1 x_2^2\n",
    "\\end{equation}\n",
    "\n",
    "Therefore compute 7 new features: 3 new ones for the quadratic terms and 4 new ones for the cubic terms.\n",
    "\n",
    "Create new arrays by stacking $x$ and the new 7 features (in the order $x_1x_1, x_2x_2, x_1x_2, x_1x_1x_1, x_2x_2x_2, x_1x_1x_2, x_1x_2x_2$). \n",
    "In particular create `x_new_quad` by additionally stacking $x$ with the quadratic features, and `x_new_cubic` by additionally stacking $x$ with the quadratic and the cubic features.\n",
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200aed3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXn0rvSM3uGD",
    "outputId": "09a90b76-0b2e-4b6c-c75f-8df4dd673954"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(\n",
    "    n_samples=700,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=.3,\n",
    "    random_state=89,\n",
    ")\n",
    "X = np.hstack([np.ones_like(X[:, [0]]), X])\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=200, random_state=42)\n",
    "\n",
    "sns.scatterplot(x=X[:, 1], y=X[:, 2], hue=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc0176",
   "metadata": {
    "id": "zicVdhc73uGF"
   },
   "outputs": [],
   "source": [
    "def get_polynomial(X, degree):\n",
    "    \"\"\"\n",
    "    Given an initial set of features, this function computes the polynomial features up to the given degree.\n",
    "\n",
    "    Args:\n",
    "        X: the initial features matrix of shape (n_samples, 3) where the first column is the bias term\n",
    "        degree: the degree of the polynomial\n",
    "\n",
    "    Returns:YOUR\n",
    "        X: the final polynomial features\n",
    "    \"\"\"\n",
    "    if degree < 2:\n",
    "        return X\n",
    "\n",
    "    features = np.ones(X.shape[0])\n",
    "\n",
    "    #####################################################\n",
    "    ##                 YOUR CODE HERE                  ##\n",
    "    #####################################################\n",
    "\n",
    "    for i in range(1, degree + 1):\n",
    "        for j in range(i + 1):\n",
    "            #print(\"x1^{} * x2^{}\".format(i - j, j))\n",
    "            features = np.column_stack((features, X[:, 1] ** (i - j) * X[:, 2] ** j))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca7ebf",
   "metadata": {
    "id": "vzyJ450Z3uGF"
   },
   "source": [
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a2aee",
   "metadata": {
    "id": "D7pukmkA3uGF"
   },
   "outputs": [],
   "source": [
    "x_new_quad = get_polynomial(X, degree=2)\n",
    "x_new_cubic = get_polynomial(X, degree=3)\n",
    "print(x_new_quad.shape, x_new_cubic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a43fe0",
   "metadata": {
    "id": "iFlwv5JY3uGF"
   },
   "source": [
    "Now use the gradient ascent optimization algorithm to learn the models by maximizing the log-likelihood, both for the case of `x_new_quad` and `x_new_cubic`.\n",
    "\n",
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea40537",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFTkk32y3uGF",
    "outputId": "1177abc2-7d88-423e-f7c6-07e29293abfa"
   },
   "outputs": [],
   "source": [
    "n_iter = 50\n",
    "model_lin = LogisticRegression(num_features=X.shape[1])\n",
    "log_l_history,_ = fit(model_lin, X, y, lr=0.5, num_steps=n_iter)\n",
    "\n",
    "# Initialize model, in case of quadratic features\n",
    "model_quad = LogisticRegression(num_features=x_new_quad.shape[1])\n",
    "log_l_history_quad,_ = fit(model_quad, x_new_quad, y, lr=0.5, num_steps=n_iter)\n",
    "\n",
    "# Initialize model, in case of quadratic and cubic features\n",
    "model_cubic = LogisticRegression(num_features=x_new_cubic.shape[1])\n",
    "log_l_history_cubic,_ = fit(model_cubic, x_new_cubic, y, lr=0.5, num_steps=n_iter)\n",
    "\n",
    "log_l = np.stack([log_l_history, log_l_history_quad, log_l_history_cubic])\n",
    "\n",
    "log_l_df = pd.DataFrame(log_l.T, columns=[\"Linear\", \"Quadratic\", \"Cubic\"])\n",
    "sns.lineplot(data=log_l_df, markers=True).set(\n",
    "    xlabel=\"Iterations\", ylabel=\"Log Likelihood\", title=\"Log Likelihood History for Different Models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60667ee",
   "metadata": {
    "id": "Zy_2fRVP3uGG"
   },
   "source": [
    "### **2.2: Plot the computed non-linear boundary** \n",
    "\n",
    "First, define a boundary_function to compute the boundary equation for the input feature vectors $x_1$ and $x_2$, according to estimated parameters theta, both in the case of quadratic (theta_final_quad) and of quadratic and cubic features (theta_final_cubic). Refer for the equation to the introductory part of Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcebc714",
   "metadata": {
    "id": "fd4r2Z3z3uGG"
   },
   "outputs": [],
   "source": [
    "def boundary_function(x1_vec, x2_vec, theta_final, degree):\n",
    "    \"\"\"\n",
    "    This function computes the boundary function for the given theta_final and degree.\n",
    "\n",
    "    Args:\n",
    "        x1_vec: the x1 vector\n",
    "        x2_vec: the x2 vector\n",
    "        theta_final: the final theta\n",
    "        degree: the degree of the polynomial\n",
    "\n",
    "    Returns:\n",
    "        x1_vec: the x1 vector\n",
    "        x2_vec: the x2 vector\n",
    "        f: the boundary function\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a meshgrid for x1 and x2 values\n",
    "    x1_vec, x2_vec = np.meshgrid(x1_vec, x2_vec)\n",
    "    \n",
    "    # Flatten the meshgrid to pass as input to get_polynomial\n",
    "    X_flattened = np.c_[np.ones(x1_vec.size), x1_vec.ravel(), x2_vec.ravel()]\n",
    "\n",
    "    # Transform the input data to the polynomial feature space\n",
    "    X_poly = get_polynomial(X_flattened, degree=degree)\n",
    "\n",
    "    # Compute the boundary function f\n",
    "    f = X_poly @ theta_final\n",
    "\n",
    "    return x1_vec, x2_vec, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b556291",
   "metadata": {
    "id": "n2udd0d63uGG"
   },
   "source": [
    "Now plot the decision boundaries corresponding to the theta_final_quad and theta_final_cubic solutions.\n",
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary_function(\n",
    "    X: np.ndarray, y: np.ndarray, theta: np.ndarray, degree: int, n_points: int = 200\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    This function plots the boundary function for the given theta and degree.\n",
    "\n",
    "    Args:\n",
    "        X: the input data\n",
    "        y: the input labels\n",
    "        theta: the final theta\n",
    "        degree: the degree of the polynomial\n",
    "        n_points: the number of points to plot\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    x1_vec = np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, n_points)\n",
    "    x2_vec = np.linspace(X[:, 2].min() - 1, X[:, 2].max() + 1, n_points)\n",
    "\n",
    "    x1_vec, x2_vec, f = boundary_function(x1_vec, x2_vec, theta, degree=degree)\n",
    "    mesh_shape = int(np.sqrt(f.shape[0]))\n",
    "\n",
    "    sns.scatterplot(x=X[:, 1], y=X[:, 2], hue=y, legend=False)\n",
    "    plt.contour(\n",
    "        x1_vec, x2_vec, f.reshape((mesh_shape, mesh_shape)), colors=\"red\", levels=[0]\n",
    "    )\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plot_boundary_function(X, y, model_lin.parameters, degree=1)\n",
    "plt.title(\"Decision Boundary for Quadratic Features\")\n",
    "plt.subplot(1,3,2)\n",
    "plot_boundary_function(X, y, model_quad.parameters, degree=2)\n",
    "plt.title(\"Decision Boundary for Quadratic Features\")\n",
    "plt.subplot(1,3,3)\n",
    "plot_boundary_function(X, y, model_cubic.parameters, degree=3)\n",
    "plt.title(\"Decision Boundary for Cubic Features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12908fe6",
   "metadata": {},
   "source": [
    "**Polynomial degree and overfitting**\n",
    "\n",
    "As the polynomial degree increases, the decision boundary becomes more and more complex. This can lead to overfitting, i.e. the model learns the training data too well, and it is not able to generalize to new data. This is a common problem in machine learning, and it is important to be able to detect it.\n",
    "\n",
    "In order to detect overfitting, we can split the dataset into a training set and a test set. The training set is used to learn the model, while the test set is used to evaluate the model performance on new data. If the model performs well on the training set, but it performs poorly on the test set, then we have overfitting.\n",
    "\n",
    "In this exercise, you are asked to plot the training and test accuracy as a function of the polynomial degree. Consider all the polynomial degrees from 1 to 20. For each polynomial degree, learn the model on the training set, and evaluate the accuracy on both the training and the test set. Additionally, visualize the decision boundary for the polynomials that give the **best** and the **worst** test accuracy for $\\texttt{degree} \\geq 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def fit_polynomials(X, y, X_test, y_test, degrees, lr, num_steps, architecture = LogisticRegression):\n",
    "    \"\"\"\n",
    "    This function fits a logistic regression model for each degree in the degrees list.\n",
    "    \"\"\"\n",
    "    X = normalize(X)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    thetas = []\n",
    "    accuracy_scores_train, accuracy_scores_test = [], []\n",
    "    for degree in tqdm(degrees):\n",
    "        x_new = get_polynomial(X, degree=degree)\n",
    "\n",
    "        model = architecture(num_features=x_new.shape[1])\n",
    "        fit(model, x_new, y, lr=lr, num_steps=num_steps)    \n",
    "\n",
    "        thetas.append(model.parameters)\n",
    "        y_hat_train = model.predict(x_new) > 0.5\n",
    "        accuracy_scores_train.append(accuracy_score(y, y_hat_train))\n",
    "        y_hat_test = model.predict(get_polynomial(X_test, degree=degree)) > 0.5\n",
    "        accuracy_scores_test.append(accuracy_score(y_test, y_hat_test))\n",
    "        \n",
    "    return thetas, accuracy_scores_train, accuracy_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28971c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(1, 20)\n",
    "np.random.seed(42)\n",
    "thetas, accuracy_scores_train, accuracy_scores_test = fit_polynomials(\n",
    "    X, y, X_val, y_val, degrees=degrees, lr=0.5, num_steps=500, architecture=LogisticRegression\n",
    ")\n",
    "sns.lineplot(x=degrees, y=accuracy_scores_train, label=\"Train\")\n",
    "sns.lineplot(x=degrees, y=accuracy_scores_test,  label=\"Test\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.xticks(degrees)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0052d",
   "metadata": {},
   "source": [
    "Plot the best and the worst decision boundaries for $\\texttt{degree} \\geq 2$.\n",
    "\n",
    "--------------------------------------------\n",
    "**Write your code below this line**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c483b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot worst model\n",
    "worst_model_idx = np.argmin(accuracy_scores_test[1:])\n",
    "degree = degrees[worst_model_idx + 1]\n",
    "\n",
    "\n",
    "x_new = get_polynomial(X, degree=degree)\n",
    "model = LogisticRegression(num_features=x_new.shape[1])\n",
    "fit(model, x_new, y, lr=0.5, num_steps=500)  \n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plot_boundary_function(X, y, model.parameters, degree=degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa71ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot best model\n",
    "best_model_idx = np.argmax(accuracy_scores_test[1:])\n",
    "degree = degrees[best_model_idx + 1]\n",
    "\n",
    "\n",
    "x_new = get_polynomial(X, degree=degree)\n",
    "model = LogisticRegression(num_features=x_new.shape[1])\n",
    "fit(model, x_new, y, lr=0.5, num_steps=500)  \n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plot_boundary_function(X, y, model.parameters, degree=degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eae9e4",
   "metadata": {
    "id": "rQGJhknOE3-U"
   },
   "source": [
    "#### **Report** \n",
    "Write now your considerations. Discuss in particular:\n",
    "1. Look back at the plots you have generated. What can you say about the differences between the linear, quadratic, and cubic decision boundaries? Can you say if the model is improving in performances, increasing the degree of the polynomial? Do you think you can incur in underfitting increasing more and more the degree?\n",
    "2. Look at the plot of the training and test accuracy as a function of the polynomial degree. What can you say about the differences between the training and test accuracy? What can you say about the differences between the best and the worst test accuracy? In general, is it desirable to have a very complex decision boundary, i.e. a very high degree of the polynomial? Discuss and motivate your answer. \n",
    "3. In general what are some properties of the dataset that makes it more prone to overfitting? Discuss their impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abefa0",
   "metadata": {
    "id": "9C3gOx5_E3-U"
   },
   "source": [
    "-------------------------------------------------------\n",
    "\n",
    "\n",
    "**WRITE YOUR ANSWER HERE:**\n",
    "\n",
    "### **Considerations on Polynomial Decision Boundaries and Model Complexity**\n",
    "\n",
    "1. Examining the plots for linear, quadratic, and cubic decision boundaries, we can notice that:\n",
    "   - **wtih degree = 1**: the decision boundary is a straight line, providing a basic separation between classes. It captures only the simplest relationships, which can lead to underfitting when data is non-linear or complex. In fact, in our case, as the classes are not linearly separable, this model performs poorly.\n",
    "   - **wtih degree = 2**: the quadratic model is able to capture more complex patterns by introducing a curved boundary that better accommodates non-linear data. The performances of our model increases a lot compared to the linear one, it almost perfectly separates the two classes.\n",
    "   - **with degree = 3**: the boundary curve is even more flexible, which on the one and enables further curvature that can fit more intricate structures in the data, on the other hand tho, it can lead to overfitting. In our case, the cubic model is able to separate the two classes even slightly better than the quadratic one, but it is likely to overfit the training data.\n",
    "\n",
    "   Generally we can say that if we chose a too high polynomial degree, the model starts to capture noise rather than meaningful patterns, leading to poor generalization on new data (the model will have a high variance).\n",
    "\n",
    "   **Underfitting**: It’s unlikely to encounter underfitting by increasing the degree further; instead, overfitting becomes a concern. As the degree rises, the model can fit the training data almost perfectly, but this high degree of flexibility often fails to generalize, reducing performance on test data. An overfitted model will on the one hand have a low bias, but on the other hand, a high variance. We should aim for a balance between bias and variance.\n",
    "\n",
    "2. Training and Test Accuracy as a Function of Polynomial Degree\n",
    "   - We can notice that the training accuracy reached its peak at degree 2, a huge improvement compared to the linear model. However, keeping to increase the degree further did not lead to additional improvements in accuracy, it instead got a little bit worse before stabilizing to a value. This behavior indicates that the quadratic model has already captured the main patterns in the data, and that further complexity is not beneficial.\n",
    "\n",
    "   - We clearly see that testing accuracy tends to increase initially, peaks at an optimal degree, and then declines as the degree increases further, finally stabilizing at a value lower than the peak. This behavior is typical of overfitting, where the model captures noise rather than meaningful patterns. In our case, the best test accuracy is achieved at degree 2. Where the model captures essential patterns without overfitting noise. In contrast, we can observe that at degree 5 we get the worst test accuracy. By plotting the decision boundaries we can clearly see that the boundary in the worst model is much more complex curve that tries to fit the training data as closely as possible, this will lead to poor generalization on new data.\n",
    "\n",
    "   It is not always desirable to have a very complex decision boundary, as it will likely lead to overfitting. The ideal polynomial degree is the one that captures the main patterns in the data without fitting noise, thus a balance between simplicity and complexity is desirable. A very high degree is usually not ideal for generalization.\n",
    "\n",
    "3. Dataset Properties That Increase Overfitting Risks\n",
    "Certain dataset characteristics tend to make overfitting more likely:\n",
    "   - **High Dimensionality**: When datasets have many features relative to sample size, models can find spurious correlations or noise, leading to overfitting.\n",
    "   - **Small Sample Size**: With fewer samples, a model is more prone to capture noise rather than general patterns.\n",
    "   - **Noise in Labels or Features**: High noise introduces random variations that the model may interpret as genuine patterns, especially with higher polynomial degrees.\n",
    "   - **Class Imbalance**: Imbalanced classes (when a class has many more data samples than the other) can cause a model to learn to fit the majority class better, potentially ignoring the minority class or overfitting to noise in the minority class.\n",
    "\n",
    "   These properties exacerbate overfitting by causing the model to \"memorize\" rather than \"learn\", which degrades test accuracy. Employing regularization techniques, reducing model complexity, or increasing training data size (if possible, of course) can help with overfitting.\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f456b6e",
   "metadata": {},
   "source": [
    "### **2.4: Weight Penalization**\n",
    "\n",
    "Look at how complicated the decision boundaries become as you increase the degree. Can we improve this and prevent overfitting?\n",
    "When dealing with overfitting one frequent solution is to use a weigth penalization technique like L2 or L1 penalization. \n",
    "\n",
    "In our case we'll use L2 regularization. In this way the regularized likelihood will be:\n",
    "$$\n",
    "\\texttt{Likelihood}_{reg}(\\theta) = \\texttt{Likelihood}(\\theta) - \\frac{\\lambda}{2n} \\sum^n_i \\theta_i^2\n",
    "$$\n",
    "Thus we can derive the update rule as:\n",
    "\\begin{equation}\n",
    "\\theta_j:= \\theta_j + \\alpha( \\frac{\\partial l(\\theta_j)}{\\partial \\theta_j} -  \\frac{\\partial}{\\partial \\theta_j} \\left( \\frac{\\lambda}{2} \\theta_j^2 \\right ) )\n",
    "\\end{equation}\n",
    "\n",
    "Calculating the second term of the update rule it's just a matter of analytically solving a simple gradient, do it, and then implement it by extending the `LogisticRegression` class:\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "**Fill in the code in `libs/models/logisic_regression_penalized.py`**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.models import LogisticRegressionPenalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19371cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(\n",
    "    n_samples=500, \n",
    "    n_features=100, \n",
    "    n_informative=50, \n",
    "    n_redundant=25, \n",
    "    n_classes=2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "np.random.seed(42)\n",
    "\n",
    "lr = LogisticRegression(X.shape[1])\n",
    "likelihood_history, val_loss_history = fit(lr, X_train, y_train, X_val, y_val, lr=1e-2, num_steps=200)\n",
    "\n",
    "penalized_lt = LogisticRegressionPenalized(X.shape[1], 2)\n",
    "pen_history, pen_val_history = fit(penalized_lt, X_train, y_train, X_val, y_val, lr=1e-2, num_steps=200)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(-likelihood_history[2:], label=\"Train\", color=\"violet\")\n",
    "plt.plot(val_loss_history[2:], label=\"Test\", color='teal')\n",
    "plt.plot(-pen_history[2:], label=\"Train - penalized\", color=\"violet\", linestyle=\"--\")\n",
    "plt.plot(pen_val_history[2:], label=\"Test - penalized\", color=\"teal\", linestyle=\"--\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Non-penalized\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bee8a3",
   "metadata": {},
   "source": [
    "Now, evaluate the Penalized Logistic Regression for each value of $\\lambda \\in [0,3]$ and find the one that performs the best: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.arange(0, 3, 0.1)\n",
    "losses = []\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    ##############################################\n",
    "    ###         COMPLETE THIS FOR-LOOP         ###\n",
    "    ##############################################\n",
    "    penalized_lt = LogisticRegressionPenalized(X.shape[1], lambda_)\n",
    "    pen_history, pen_val_history = fit(penalized_lt, X_train, y_train, X_val, y_val, lr=1e-2, num_steps=200)\n",
    "    losses.append(pen_val_history[-1])\n",
    "\n",
    "if len(losses) > 0:\n",
    "    sns.lineplot(x=lambdas, y=losses, label=\"Validation Loss\").set(\n",
    "        xlabel=\"Lambda\", ylabel=\"Loss\", title=\"Validation Loss vs Lambda\"\n",
    "    )\n",
    "    print(f\"Best lambda: {lambdas[np.argmin(losses)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a196b",
   "metadata": {},
   "source": [
    "#### Report\n",
    "Write now your considerations. In particular:\n",
    "1. What happens when we use a non-penalized logistic regression?\n",
    "2. Observe the plot of the Train and Validation losses in the penalized vs non penalized case. In which case is the Train loss better? Can you explain why?\n",
    "3. What is the convergence rate? How is it influenced by the penalization?\n",
    "\n",
    "-------------------------------------------------------\n",
    "\n",
    "\n",
    "**WRITE YOUR ANSWER HERE:**\n",
    "\n",
    "1. When we use a non-penalized logistic regression (i.e., without regularization), the model has no constraint on the magnitude of the weights. As a result:\n",
    "    - Without regularization, the model can fit the training data very closely, even capturing noise or random fluctuations. This leads to overfitting, where the model performs very well on the training data but poorly on unseen data (test set). The decision boundary can become very complex and overly adapted to the specificities of the training set, which reduces the generalization ability of the model.\n",
    "    - The model can assign large values to the weights, leading to a more complicated decision boundary. This makes the model more flexible, but it can also make the model more sensitive to slight changes in the data. As the degree of the polynomial features increases, this effect becomes even more pronounced.\n",
    "    - The lack of penalization means that features with higher numerical ranges can dominate the decision-making process, even if they don't carry the most important information for classification. This can cause the model to overemphasize less relevant features.\n",
    "\n",
    "2. In the non-penalized model, the train loss is lower than the penalized model, this is because the non-penalized model will fit the training data more closely, but this will make the model capture noise and overfit the data. Talking about validation loss, we can clearly see that the penalized model performs better. The loss of the non-penalized model does actually start to increase after a certain iteration, this is a clear sign of overfitting.\n",
    "\n",
    "3. The convergence rate refers to how quickly an algorithm approaches its optimal solution during training. In logistic regression, the convergence rate is influenced by factors like the learning rate, the complexity of the model, and penalization. In the context of regularization, as we are adding a penalty term to the loss function, the convergence rate can be influenced by the strength of the regularization parameter λ. Adding an L2 penalty typically helps stabilize the learning process, reducing oscillations or erratic behavior during training. In fact, in our case, we can see that looking at the graph above, the penalized model converges faster than the non-penalized one. We can in fact see that the training loss of the penalized model stabilizes after 75 iterations, while the training loss of the non-penalized model keeps decreasing. If we increase the number of iterations, we will see that the non-penalized model's train loss will converge, but much slower (for what we said before, the non-penalized model's loss will converge to a lower value).\n",
    "\n",
    "In conclusion, we can say that penalization helps us to find a better trade-off between bias and variance. By adding a penalty term, regularization reduces the model's flexibility, which prevents overfitting and helps with generalization, thus reducing variance. However, this also introduces some bias by constraining the model from fitting the data too perfectly.\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faba031",
   "metadata": {
    "id": "JTeuyG-S3uGH",
    "tags": []
   },
   "source": [
    "## 3: **Multinomial Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107612c",
   "metadata": {
    "id": "KS-AS1PUE3-V"
   },
   "source": [
    "### **3.1: Softmax Regression Model**\n",
    "\n",
    "In the multinomial classification we generally have $K>2$ classes. So the label for the $i$-th sample $X_i$ is $y_i\\in\\{1,...,K\\}$, where $i=1,...,N$. The output class for each sample is estimated by returning a score $s_i$ for each of the K classes. This results in a vector of scores of dimension K. \n",
    "In this exercise we'll use the *Softmax Regression* model, which is the natural extension of *Logistic Regression* for the case of more than 2 classes. The score array is given by the linear model:\n",
    "\n",
    "\\begin{align*}\n",
    "s_i =  X_i \\theta\n",
    "\\end{align*}\n",
    "\n",
    "Scores may be interpreted probabilistically, upon application of the function *softmax*. The position in the vector with the highest probability will be predicted as the output class. The probability of the class k for the $i$-th data sample is:\n",
    "\n",
    "\\begin{align*}\n",
    "p_{ik} = \\frac{\\exp(X_i \\theta_k)}{\\sum_{j=1}^K(X_i \\theta_j))}\n",
    "\\end{align*}\n",
    "\n",
    "We will adopt the *Cross Entropy* loss and optimize the model via *Gradient Descent*. \n",
    "In the first of this exercise we have to: \n",
    "-    Write the equations of the Cross Entropy loss for the Softmax regression model;\n",
    "-    Compute the equation for the gradient of the Cross Entropy loss for the model, in order to use it in the gradient descent algorithm.\n",
    "\n",
    "#### A bit of notation\n",
    "\n",
    "*  N: is the number of samples \n",
    "*  K: is the number of classes\n",
    "*  X: is the input dataset and it has shape (N, H) where H is the number of features\n",
    "*  y: is the output array with the labels; it has shape (N, 1)\n",
    "*  $\\theta$: is the parameter matrix of the model; it has shape (H, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c8304",
   "metadata": {
    "id": "xHX1s7jp3uGI"
   },
   "source": [
    "--------------------------------------------\n",
    "**Write you equation below this line**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10170a74",
   "metadata": {
    "id": "ixObV4w43uGI"
   },
   "source": [
    "\\begin{align*}\n",
    "L(\\theta) = -\\sum_{i=1}^N \\sum_{k=1}^K y_{ik} \\log(p_{ik})\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "Loss(\\theta) = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^K y_{ik} \\log(p_{ik})\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} L(\\theta) = -\\sum_{i=1}^N \\left(y_i - p_i\\right) X_i\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ccc0db",
   "metadata": {
    "id": "rMxrcWc53uGI"
   },
   "source": [
    "### **3.2: Coding**\n",
    "\n",
    "We are using the CIFAR-10 dataset for this exercise. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. It has 50,000 training images and 10,000 test images. The dataset was established by the Canadian Institute For Advanced Research (CIFAR), and it has become a standard benchmark for machine learning algorithms, especially in the area of image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e56f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "cifar_dir = \"assets/cifar10\"\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=cifar_dir, train=True, download=True, transform=transform\n",
    ")\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=cifar_dir, train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    encoded = np.zeros((len(y), num_classes))\n",
    "    for i, val in enumerate(y):\n",
    "        encoded[i, val] = 1\n",
    "    return encoded\n",
    "\n",
    "# Evaluate the accuracy of the predictions\n",
    "def compute_accuracy(predictions, true_labels):\n",
    "    correct_predictions = np.sum(predictions == true_labels)\n",
    "    total_predictions = len(true_labels)\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = [img.reshape(-1).numpy() for img, _ in train_data]\n",
    "X_train = np.array(X_train)\n",
    "y_train = [label for _, label in train_data]\n",
    "\n",
    "X_val = [img.reshape(-1).numpy() for img, _ in test_data]\n",
    "X_val = np.array(X_val)\n",
    "y_val = [label for _, label in test_data]\n",
    "\n",
    "\n",
    "# Add bias term to X\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "\n",
    "y_train_onehot = one_hot_encode(y_train)\n",
    "y_test_onehot = one_hot_encode(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f30d9",
   "metadata": {
    "id": "2RrCmafP3uGI"
   },
   "source": [
    "*Hint: consider the labels as one-hot vector. This will allow matrix operations (element-wise multiplication and summation).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63efe6",
   "metadata": {},
   "source": [
    "Now implement a classifier for Multinomial Classification using the `softmax` function. Again, implement it as a class with the methods:\n",
    "- `predict`\n",
    "- `predict_labels`\n",
    "- `likelihood` *(Here you need to implement the Cross Entropy Loss)*\n",
    "- `update_theta`\n",
    "- `compute_gradient` to compute the Jacobian $\\nabla$\n",
    "\n",
    "Note that this this you don't need to reimplement the `fit()` function since the training loop you defined above works also for a Multinomial Classifier, provided that this is structured with the previously mentioned methods.\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "**Fill in the code in `libs/models/multinomial.py` and `libs/math.py/softmax()`**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbca091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.models import SoftmaxClassifier\n",
    "from libs.optim import fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21abf9",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "*Execution can take around 10 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply gradient descent to optimize theta\n",
    "alpha = 0.01\n",
    "iterations = 500\n",
    "H, K = X_train.shape[1], 10  # number of features and number of classes\n",
    "model = SoftmaxClassifier(num_features=H, num_classes=K)\n",
    "loss_history, _ = fit(model, X_train, y_train_onehot, lr=alpha, num_steps=iterations)\n",
    "\n",
    "# Make predictions on the training and test data\n",
    "train_predictions = model.predict_labels(X_train)\n",
    "test_predictions = model.predict_labels(X_val)\n",
    "\n",
    "train_accuracy = compute_accuracy(train_predictions, y_train)\n",
    "test_accuracy = compute_accuracy(test_predictions, y_val)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46307d23",
   "metadata": {},
   "source": [
    "### **3.3: Pipeline**\n",
    "Now you're going to use `scikit-learn` library to build a pipeline of operations to redo everything we've done so far in the homework. First we have loaded the required modules and the penguins dataset.\n",
    "\n",
    "---\n",
    "\n",
    "Then here you'll build the pipeline. We need four items:\n",
    "1. The Numerical Transformer, to handle the preprocessing of numerical columns, by:\n",
    "    - Imputing missing values with their mean\n",
    "    - Enrich the features with a 3-rd degree polynomial expansion\n",
    "    - Scaling of the features to $\\mu=0, \\sigma=1$\n",
    "2. The Categorical Transformer, to handle the preprocessing of categorical values, by:\n",
    "    - Imputing the missing values with the most frequent value\n",
    "    - Encode the features in a one-hot vector.\n",
    "3. The Preprocessor: a ColumnTransformer that distributed the numerical columns to the numerical transformer and the categorical columns to the categorical tranformer.\n",
    "4. The final Pipeline, which contains the preprocessor and the classfier of your choice (in this case `KNeighborsClassifier`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('assets/train.csv')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data.drop('species', axis=1), data.species, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['bill_length', 'bill_depth', 'flipper_length', 'body_mass']\n",
    "categorical_cols = ['island', 'sex']\n",
    "\n",
    "##############################################\n",
    "###          FILL IN THIS CODE           #####\n",
    "##############################################\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "## Import everything you need here\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "numeric_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    PolynomialFeatures(degree=3, include_bias=False),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "categoric_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),\n",
    "    OneHotEncoder(drop='if_binary', handle_unknown='error')\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numerical_cols),\n",
    "    ('cat', categoric_transformer, categorical_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "pipe = make_pipeline(preprocessor, KNeighborsClassifier(n_neighbors=4))\n",
    "if len(pipe.named_steps)>0:\n",
    "    display(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20cc4b",
   "metadata": {},
   "source": [
    "Now, we can use this pipeline to preprocess the input data and fit a classifier. Leveraging `scikit-learn`'s pipelines allows you to:\n",
    "- Define the entire chain of operations in a structured way, which is especially useful for cleaning and transforming data.\n",
    "- Separate the definition of operations from their execution, creating a clean and organized workflow.\n",
    "\n",
    "This approach makes it easier to manage complex preprocessing steps while maintaining readability and clarity in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1829845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd86ac",
   "metadata": {},
   "source": [
    "This is nice but can we improve it? In defining the pipeline you certainly used some fixed hyperparameters, for example the number of neighbors or the degree of the polynomial expansion.\n",
    "\n",
    "First, let's look at the list of hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcfd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = pipe.get_params()\n",
    "for hp, val in hparams.items():\n",
    "    if type(val) not in [int, float, str]:\n",
    "        continue\n",
    "    print(f\"{hp}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99eea0",
   "metadata": {},
   "source": [
    "Some of these hyperparameters are set to their default values, while others are explicitly defined. However, any data scientist knows that hyperparameters should not be arbitrarily chosen; instead, they should be optimized through **Cross-Validation**.\n",
    "\n",
    "We can leverage the compositionality of `scikit-learn` by incorporating the pipeline into a `GridSearchCV` class. This allows you to easily define a grid of parameters to val and automatically perform cross-validation over the combinations.\n",
    "\n",
    "Choose at least 2 values for at least 3 hyperparameters. val their impact on the model and find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'columntransformer__cat__onehotencoder__drop': [None, 'first', 'if_binary'],\n",
    "    'kneighborsclassifier__n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "    'columntransformer__num__polynomialfeatures__degree': [1, 2, 3, 4, 5],\n",
    "}\n",
    "# Set up GridSearchCV with cross-validation\n",
    "pipe_cv = GridSearchCV(pipe, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "if pipe_cv is not None:\n",
    "    pipe_cv.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best score: {pipe_cv.best_score_}\")\n",
    "    for hp, val in pipe_cv.best_params_.items():\n",
    "        print(f\"{hp}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a24ba",
   "metadata": {},
   "source": [
    "`GridSearchCV` doesn't only find the best combination of hyperparmeters, but it also refits the model with the best hyperparameters it finds. Let's val this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76df863",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe_cv.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c4ac8",
   "metadata": {},
   "source": [
    "#### Report\n",
    "1. How many combinations has your gridsearch tried?\n",
    "2. Make a plot with the results of your hyperparameter grid\n",
    "3. Do you notice any trend in the performance of certain hyperparameters?\n",
    "4. Do the classifiers obtain the same accuracy on train and val sets? If not, try to give an explanation.\n",
    "5. With the choice of hyperparameters you made, do you notice any trade-off between accuracy and compute power? Show with a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ce062",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(pipe_cv.cv_results_)\n",
    "cv_res.columns = [col.split('__')[-1] for col in cv_res.columns]\n",
    "\n",
    "\n",
    "##############################################\n",
    "###                YOUR CODE HERE         ####\n",
    "##############################################\n",
    "\n",
    "# Print the number of combinations tried\n",
    "param_counts = [len(values) for values in param_grid.values()]\n",
    "total_combinations = np.prod(param_counts)\n",
    "print(f\"1. Number of combinations tried: {total_combinations}\")\n",
    "\n",
    "# Print train and test accuracy\n",
    "a_y_pred = pipe_cv.predict(X_train)\n",
    "print(\"Train accuracy: \", accuracy_score(y_train, a_y_pred))\n",
    "print(\"Test accuracy: \", accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e79bf",
   "metadata": {},
   "source": [
    "### Plot 1\n",
    "This first plot shows the mean test score that the model obtained for each combination of hyperparameters.\n",
    "Data is represented in the following way:\n",
    "- The x-axis represents the n_neighbors parameter\n",
    "- The color of the marker represents the degree of the polynomial expansion\n",
    "- The shape of the marker represents the value of the drop parameter for the one hot encoding\n",
    "    - Circle for drop=None (no drop of categories)\n",
    "    - Square for drop='first' (the first category is dropped)\n",
    "    - Triangle for drop='if_binary' (the first category is dropped if the category is binary)\n",
    "- The y-axis represents the mean test score obtained by the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "mean_test_scores = cv_res['mean_test_score']\n",
    "neighbors = cv_res['n_neighbors']\n",
    "degree = cv_res['degree']\n",
    "total_drop_values = cv_res['drop'].unique()\n",
    "\n",
    "# Define the markers, their size and their transparency (to help visualize the data)\n",
    "markers = ['o', 's', '^']\n",
    "sizes = [30, 40, 60]\n",
    "alphas = [1, 0.5, 0.7]\n",
    "marker_map = {val: markers[i % len(markers)] for i, val in enumerate(total_drop_values)}\n",
    "size_map = {val: sizes[i % len(sizes)] for i, val in enumerate(total_drop_values)}\n",
    "alpha_map = {val: alphas[i % len(alphas)] for i, val in enumerate(total_drop_values)}\n",
    "print(\"onehotencoder__drop map to marker:\", marker_map)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for val in total_drop_values:\n",
    "    if val is not None:\n",
    "        subset = cv_res[cv_res['drop'] == val]\n",
    "    else:\n",
    "        subset = cv_res[cv_res['drop'].isnull()]\n",
    "    \n",
    "    scatter = plt.scatter(subset['n_neighbors'], subset['mean_test_score'], c=subset['degree'], cmap='viridis', marker=marker_map[val], label=val, linewidth=0.5, alpha=alpha_map[val], s=size_map[val])\n",
    "\n",
    "# Add the colorbar for the polynomial degree\n",
    "norm = plt.Normalize(vmin=cv_res['degree'].min(), vmax=cv_res['degree'].max())\n",
    "cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap='viridis'), ax=plt.gca(), label='polynomial degree')\n",
    "cbar.set_ticks(cv_res['degree'].unique())\n",
    "\n",
    "\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('mean_test_score')\n",
    "plt.title('Plot 1: Hyperparameter Grid Search Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb27359",
   "metadata": {},
   "source": [
    "### Plot 2\n",
    "In this plot, we are averaging the mean score for each value of the n_neighbors parameter. This way we can show the trend of the accuracy as the n_neighbors parameter changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by 'n_neighbors' and calculate the mean of 'mean_test_score'\n",
    "cv_res_mean = cv_res.groupby(\"n_neighbors\")[\"mean_test_score\"].mean().reset_index()\n",
    "\n",
    "# Plot the mean values\n",
    "plt.scatter(cv_res_mean[\"n_neighbors\"], cv_res_mean[\"mean_test_score\"], linewidth=0.5, alpha=0.6)\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"Mean Test Score\")\n",
    "plt.title(\"Plot 2: average mean_test_score per n_neighbors\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f618d54",
   "metadata": {},
   "source": [
    "### Plot 3\n",
    "In this plot, we are going to plot, for each combination of hyperparameters, the compute time it took to fit the model. This way we can see if there is a trade-off between accuracy and compute power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da42ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_time = cv_res['mean_fit_time']\n",
    "accuracy = cv_res['mean_test_score']\n",
    "\n",
    "# find the highest accuracy\n",
    "best_accuracy = accuracy[np.argmax(accuracy)]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(fit_time[accuracy < best_accuracy], accuracy[accuracy < best_accuracy], c='red')\n",
    "plt.scatter(fit_time[accuracy == best_accuracy], accuracy[accuracy == best_accuracy], c='green')\n",
    "\n",
    "plt.xlabel('Training Time (seconds)')\n",
    "plt.ylabel('Mean Test Score')\n",
    "plt.title('Trade-off between Accuracy and Compute Power')\n",
    "plt.show()\n",
    "\n",
    "# The green boints are the models that achieved the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c673d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can see, more than a model achieves the best accuracy, so we want to pick the one that required the lowest training time.\n",
    "# let's print the hyperparameters of the best model\n",
    "\n",
    "best_accuracy_models = cv_res[accuracy == best_accuracy]\n",
    "best_overall_model = best_accuracy_models[fit_time == np.min(fit_time[accuracy == best_accuracy])]\n",
    "\n",
    "# Print the best overall model hyperparameters\n",
    "print(\"Best overall model hyperparameters:\")\n",
    "for hp, val in best_overall_model.iloc[0].items():\n",
    "    print(f\"{hp}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e95b8",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "\n",
    "**Write your answer below this line**\n",
    "\n",
    "1. We tried a total of 180 combinations, testing the following values for the hyperparameters:\n",
    "   - `kneighborsclassifier__n_neighbors`: [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "   - `columntransformer__num__polynomialfeatures__degree`: [1, 2, 3, 4, 5, 6, 7]\n",
    "   - `columntransformer__cat__onehotencoder__drop`: ['first', 'if_binary', None]\n",
    "\n",
    "2. The plot is shown above.\n",
    "\n",
    "3. Trend in the performance of certain hyperparameters:\n",
    "   From the grid search results and the scatter plot, we can observe that the model achieved the best performance on the test set with the parameters `n_neighbors=8`, `degree=1`, and `drop='if_binary'`, achieving a mean test score of 80.1%. In our case, as the scoring metric for the GridSearchCV is accuracy, it means that these parameters are the ones that maximize the accuracy on our dataset. The mean test score is the average score obtained on the test set across all cross-validation folds.\n",
    "\n",
    "   - **`n_neighbors`:** We can observe that, generally speaking (by looking at plot 2 and considering, for each value of n_neighbors, the average score obtained by varying the other two parameters), the score tends to increase up to n_neighbors=7 and then it stabilizes. Looking at plot 1 we can clearly see that best overall score is achieved with n_neighbors=8, with the other two parameters set to their optimal values.\n",
    "\n",
    "   - **`polynomial degree`:** The model performs better when the polynomial degree (the degree of the polynomial features we created to augment the dataset) is 1, which means that no new polynomial features are added. We can clearly see from plot 1 that `polynomialfeatures__degree=1` outperforms in almost every case (the round marker is always at the top). As we try to increase the value, we can see that the performance on our dataset decreases, suggesting that increasing the polynomial degree adds unnecessary complexity, which leads to overfitting rather than improved performance.\n",
    "\n",
    "   - **`onehotencoder drop` (OneHotEncoder):** One Hot Encoding is a technique used to convert categorical variables into a format that could be provided to machine learning algorithms, it transforms each category value into a binary vector witn n elements (with n equal to the number of categories). In a vector, only one element will be 1 (hot) and the others will be 0 (cold). The drop parameter specifies if and which category should be dropped to avoid issues with multicollinearity (the dummy variable trap). This is very useful for models that assume independence among features.\n",
    "   In our case, the best performance is achieved when drop='if_binary', which tells the encoder to drop the first category in each feature with two categories.\n",
    "   Looking at the plot, we can see that drop='if_binary' (the triangle marker) achieves the best performance in most cases, suggesting that there is some multicollinearity in the dataset. In fact, if we look at the train.csv file (the dataset used), we can see that 'sex' is a binary feature (it has two categories, and it is clear that if one is 1, the other is 0. Leaving them both in the dataset would lead to multicollinearity, which can cause issues with the model's performance. The other categorical feature is 'island' which has 3 categories, if we drop the first one, we are losing information. This is why with drop='first' the model tends to perform worse in the majority of cases (in the plot, the square marker is always at the bottom of the other markers with the same color).\n",
    "\n",
    "\n",
    "4. The accuracy on the train set is 82.6%, while the accuracy on the eval set is 80.4%. The differenct not big and it does not indicate that some strong overfitting is happening. When the gap between the train and test set is small, it means that the model generalizes well. Obtaining the same exact accuracy on both the train and test set is typically not achievable, especially in real world scenarios.\n",
    "\n",
    "5. In our plot (plot 3), the green point represent the model that achieved the best mean_test_score. To answer the question, yes, there is a very slight trade-off between accuracy and compute power, but it's very subtle. The difference in computational power (in term of execution time) between the most complex and the simplest model is very small. We ran the training 10 times for each model, and averaged the execution time to get a more reliable estimate. In our test, the system toom about 5ms to train the simplest model, and 30ms to train the most complex one. We can definitely say that there is a difference, as the most complex model took about 6 times longer to train, but the difference is extremely small, also because our dataset was very small. With a much bigger dataset the difference would probably have been more evident.\n",
    "The model that ahieved the best accuracy took about 17ms to train, which is in the average between the two extremes. This suggests that increasing the model's complexity initially does give an improvement in the model's performance, but if we keep increasing the complexity, the model will start to overfit the training data, which will lead to a decrease in performance on the test set. The best model is the one that finds the right balance between complexity and simplicity.\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a3c8e",
   "metadata": {
    "id": "OwYcUWZnrDgP"
   },
   "source": [
    "## **4: Debugging a CNN with Shape Errors**\n",
    "\n",
    "You are provided with a CNN model intended to classify images from the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. However, the model contains shape mismatches between layers due to intentional errors. Your first task is to identify and fix these errors to make the model functional.*testo in corsivo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941ac13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:41:52.574290Z",
     "start_time": "2024-11-14T15:41:45.875861Z"
    },
    "id": "9FPQbEr9rZIL"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d671f",
   "metadata": {
    "id": "rWx5oYSet38n"
   },
   "source": [
    "### 4.1: Split the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0881a353",
   "metadata": {
    "id": "IviQYVL0rbZ5"
   },
   "source": [
    "Instructions:\n",
    "\n",
    "* Define the Split Sizes:\n",
    "Calculate the sizes for the training and validation datasets. Allocate ***80% of the training*** data for the training set and *20% for the validation set*.\n",
    "\n",
    "* Split the Dataset:\n",
    "Use `torch.utils.data.random_split` to create the training and validation datasets from the original training dataset.\n",
    "\n",
    "* Create Data Loaders:\n",
    "Create data loaders for the training, validation, and test datasets using torch.utils.data.DataLoader with a ***batch size of 64***. Ensure that the training data is ***shuffled***.\n",
    "\n",
    "* Print the size of each dataset (train, test, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b82813",
   "metadata": {
    "id": "R3Tc8O6zrh6v"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define transformations for the data that we will use\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "#####################################################\n",
    "##              YOUR CODE HERE                     ##\n",
    "#####################################################\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "# use these names for the data loaders\n",
    "\n",
    "train_dataset, validation_dataset = data_utils.random_split(full_train_dataset, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Full dataset size: {len(full_train_dataset)}\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ba0401",
   "metadata": {
    "id": "8WWMSLDSt9j7"
   },
   "source": [
    "### 4.2: Identify and Correct Errors in the CNN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806744d",
   "metadata": {
    "id": "Laxva2bSrodv"
   },
   "source": [
    "In this exercise, you will analyze an intentionally incorrect implementation of a Convolutional Neural Network model. Your task is to identify the errors in the `PoorPerformingCNN` class and correct them to ensure the model works properly for the CIFAR-10 dataset.\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "**Fill in the code in `libs/models/poor_cnn.py**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d190e2",
   "metadata": {
    "id": "3qRf7YUAr49u"
   },
   "outputs": [],
   "source": [
    "from libs.models import PoorPerformingCNN\n",
    "\n",
    "net = PoorPerformingCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb7e81",
   "metadata": {
    "id": "CwRxKCRMsUiU"
   },
   "source": [
    "Loss Function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73233d5",
   "metadata": {
    "id": "lQ_9fKCfsV3X"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b442d3",
   "metadata": {
    "id": "RSm7aXCJuE1I"
   },
   "source": [
    "### 4.3: Training procedure\n",
    "\n",
    "In this exercise, you will complete the training and validation loop of a neural network model. Your task is to compute and store the average training loss, average validation loss, and the corresponding accuracies for each epoch.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "* **Training Phase**:\n",
    "After computing the average training loss (avg_train_loss), you need to calculate the training accuracy based on the model's predictions and append the calculated training accuracy to the train_accuracies list.\n",
    "\n",
    "* **Validation Phase**:\n",
    "After calculating the average validation loss (avg_val_loss), you need to calculate the validation accuracy based on the validation dataset and append the calculated validation accuracy to the val_accuracies list. (the same as befor but for the val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858bb39",
   "metadata": {
    "id": "RSUv7VMdsblF"
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    # Training Phase\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    #####################################################\n",
    "    ##              YOUR CODE HERE                     ##\n",
    "    #####################################################\n",
    "    \n",
    "    # After computing the average training loss (avg_train_loss), you need to calculate the training accuracy based on the model's predictions and append the calculated training accuracy to the train_accuracies list.\n",
    "    # Calculate average loss for the training epoch\n",
    "    avg_train_loss = running_loss / total_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    y_pred = torch.max(outputs.data, 1)\n",
    "    train_accuracy = (y_pred.indices == labels).sum().item() / len(labels)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    #####################################################\n",
    "    ##              END OF YOUR CODE                   ##\n",
    "    #####################################################\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Average Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Validation Phase\n",
    "    net.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_data in val_loader:\n",
    "            val_inputs, val_labels = val_data\n",
    "\n",
    "            # Forward pass\n",
    "            val_outputs = net(val_inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            total += val_labels.size(0)\n",
    "            correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ##              YOUR CODE HERE                     ##\n",
    "    #####################################################\n",
    "\n",
    "    # Calculate average loss for the validation epoch\n",
    "    avg_val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = correct / total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    #####################################################\n",
    "    ##              END OF YOUR CODE                   ##\n",
    "    #####################################################\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Average Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb35d88",
   "metadata": {
    "id": "WhmPPQf9sh37"
   },
   "source": [
    "### 4.4: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248e772",
   "metadata": {
    "id": "Bo6vZmsUsjyr"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        #####################################################\n",
    "        ##              YOUR CODE HERE                     ##\n",
    "        #####################################################\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        #####################################################\n",
    "        ##              END OF YOUR CODE                   ##\n",
    "        #####################################################\n",
    "\n",
    "print(f'Accuracy on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9755d",
   "metadata": {
    "id": "7kX4nLdOssWw"
   },
   "source": [
    "### 4.5: Report\n",
    "\n",
    "1. What challenges can class imbalance introduce when training a machine learning model?\n",
    "\n",
    "2. What are some strategies to address class imbalance in your dataset or training process?\n",
    "\n",
    "3. Why might accuracy alone be misleading as a performance measure in the presence of class imbalance\n",
    "\n",
    "4. Is the cifar-10 and imbalanced dataset? plot the number of samples for each classes inside the cifar 10 dataset.\n",
    "\n",
    "-------------------------------------------------------\n",
    "\n",
    "\n",
    "**WRITE YOUR ANSWER HERE:**\n",
    "\n",
    "1. Class imbalance can be especially problematic in machine learning because many algorithms assume that the number of samples in each class is approximately equal. When there’s a large imbalance, the model may learn to \"ignore\" the minority class, resulting in poor performance on that class and an inaccurate evaluation of the model's true capabilities. Another challenge introduced by imbalance is overfitting, infact the model may overfit to the majority class, learning its patterns too closely and failing to generalize to unseen data, especially for the minority class.\n",
    "There is high false negative rate challenge beacuse the model may mistakenly classify instances of the minority class as belonging to the majority class, leading to missed opportunities or incorrect decisions. \n",
    "2. In order to address class imbalance in a dataset we can do:\n",
    "    * **Data augmentation**: If working with images, text, or time series data, this technique  can create additional samples for the minority class, providing more training examples.\n",
    "\n",
    "    * **Resampling**:\n",
    "        * **Oversampling**: increasing the number of samples in the minority class to balance the dataset. This can be done by duplicating existing minority samples or generating synthetic samples.\n",
    "        It is best suited for situations where the minority class is too small and it works well with relatively small datasets, where adding more data is advantageous for training.\n",
    "        * **Undersampling**: reducing the number of samples in the majority class to balance it with the minority class. This can help models focus on meaningful differences between classes but may result in data loss.\n",
    "        It is suited for large datasets where reducing the number of majority samples doesn’t cause significant information loss and it is useful in cases where the majority class is extremely large relative to the minority class, as undersampling can reduce dataset size and speed up training.\n",
    "    * **Class Weights**: rather than modifying the dataset, this strategy adjust the training process so that the model \"pays more attention\" to the minority class by penalizing misclassifications more heavily. It modifies the loss function by assigning higher weights to errors involving the minority class, making those errors contribute more to the total loss. This helps the model learn the minority class patterns more effectively without needing additional samples.\n",
    "3. Accuracy alone can be misleading because it doesn’t account for how well the model performs on each individual class—particularly the minority class, which is often the focus in imbalanced datasets.\n",
    "The problem is that the majority class makes up a large portion of the data, so a model can achieve high accuracy simply by correctly predicting the majority class. For instance, in a dataset with 95% majority class and 5% minority class, a model that always predicts the majority class will still achieve 95% accuracy, so the model could have a high rate of false negatives (failing to identify minority class instances) while still achieving high accuracy.\n",
    "In order to address the above described problem, we should to look at metrics that provide insights into each class like F1 Score, Recall, ROC-AUC ecc...\n",
    "4. It is a balanced dataset, ifconsists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.The training batches contain exactly 5000 images from each class.\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd907b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load again the CIFAR-10 training dataset (just to avoid problems with the previous split)\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Get the labels for the training data\n",
    "train_labels = np.array(full_train_dataset.targets)\n",
    "\n",
    "# CIFAR-10 class labels\n",
    "class_labels = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Count the number of samples for each class\n",
    "class_counts = np.bincount(train_labels)\n",
    "\n",
    "# Plotting the number of samples for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(class_labels, class_counts, color='green')\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Number of Samples for Each Class in CIFAR-10 Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb829e",
   "metadata": {
    "id": "zgB9zLE6s04N"
   },
   "source": [
    "## **5: Improve the accuracy** (BONUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c289bcd",
   "metadata": {
    "id": "Lm7RZNOltsWf"
   },
   "source": [
    "### 5.1: Custom model\n",
    "\n",
    "After successfully debugging the model, you'll notice that the accuracy on the CIFAR-10 dataset is only around 50-60%. Your second task is to improve the model's performance.\n",
    "\n",
    "How?\n",
    "\n",
    "*   Add more convolutional layers to capture higher-level features.\n",
    "*   Use Batch Normalization\n",
    "*   Add Dropout Layers\n",
    "\n",
    "Data Augmentation:\n",
    "*   Apply transformations like random cropping, flipping, and rotation.\n",
    "\n",
    "Hint: You CAN implement already pre-existing CNN architectures (do your research). As long as it is a CNN everything is fine.\n",
    "\n",
    "By the end of this section you should return the accuracy of your model on the test dataset.\n",
    "\n",
    "NB: by better score we mean at least +10% with respect to the previous model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ea8c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:41:59.253926Z",
     "start_time": "2024-11-14T15:41:57.830672Z"
    },
    "id": "CZNA68C5s2Is"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Define transformations for the data\n",
    "transform = transforms.Compose([\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486bead",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "\n",
    "**Fill in the code in `libs/models/custom_cnn.py**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30575ec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:42:01.524157Z",
     "start_time": "2024-11-14T15:42:01.225680Z"
    },
    "id": "5TX-5wRmtIAt"
   },
   "outputs": [],
   "source": [
    "from libs.models import CustomCNN\n",
    "\n",
    "net = CustomCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba4705",
   "metadata": {
    "id": "FstE7TPStY-n"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba8675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:45:46.726359Z",
     "start_time": "2024-11-14T15:42:03.031886Z"
    },
    "id": "zDjiBMdTtVcI"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=5e-05)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb3b60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "350ceb57",
   "metadata": {
    "id": "vi3EBTq_tXYL"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7550f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:46:07.774824Z",
     "start_time": "2024-11-14T15:46:03.577024Z"
    },
    "id": "mNOjqxdwtW99"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c58dfb",
   "metadata": {
    "id": "KICkuQv9tbJk"
   },
   "source": [
    "\n",
    "### 5.2: Pretrained network\n",
    "In this exercise, you will start from scratch to adapt a pre-trained AlexNet model for the CIFAR-10 dataset.\n",
    "\n",
    "Instructions\n",
    "\n",
    "- Use torchvision.models to load a pre-trained AlexNet. Be sure to specify that the model should be pre-trained on ImageNet.\n",
    "\n",
    "- The CIFAR-10 dataset has 10 classes, so you need to update the model’s final layer to output 10 classes instead of the default 1000.\n",
    "\n",
    "- Replace the final fully connected layer in AlexNet’s classifier to output 10 classes.\n",
    "\n",
    "- To perform fine-tuning, freeze all layers except the newly added fully connected layer.\n",
    "\n",
    "- Move your model to the appropriate device (cuda if available). Define a device and ensure the model is moved to that device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ee586",
   "metadata": {
    "id": "3DsgJ10RtjPS"
   },
   "outputs": [],
   "source": [
    "# Define transformations for the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8400e",
   "metadata": {
    "id": "eLCv9hY7tkBG"
   },
   "outputs": [],
   "source": [
    "\n",
    "#####################################################\n",
    "##              YOUR CODE HERE                     ##\n",
    "#####################################################\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.alexnet(weights='IMAGENET1K_V1')\n",
    "model.classifier[6] = nn.Linear(4096, 10)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier[6].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "#####################################################\n",
    "##              END OF YOUR CODE                   ##\n",
    "#####################################################\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bfc3c4",
   "metadata": {
    "id": "5-t3hEZGtmSo"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-05)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on the test images: {100 * correct / total:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
